{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f586023b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from synference import Simformer_Fitter, load_unc_model_from_hdf5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7038020d",
   "metadata": {},
   "source": [
    "# Load and create models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fadd344",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_pop3_path = \"/home/tharvey/work/synference/libraries/grid_Yggdrasil_Chab_Burst_SFH_0.001_z_14_logN_4.4_v1.hdf5\"\n",
    "\n",
    "fitter_pop3 = Simformer_Fitter.init_from_hdf5(\n",
    "    \"Yggdrasil_Chab_Burst_SFH_0.001_z_14_logN_4.4_v1\", grid_pop3_path\n",
    ")\n",
    "\n",
    "bpass_path = \"/home/tharvey/work/synference/libraries/grid_BPASS_Chab_DenseBasis_SFH_0.01_z_14_logN_5.0_Calzetti_v3_multinode.hdf5\"  # noqa: E501\n",
    "\n",
    "fitter_bpass = Simformer_Fitter.init_from_hdf5(\n",
    "    \"BPASS_Chab_DenseBasis_SFH_0.01_z_14_logN_5.0_Calzetti_v3_multinode\", bpass_path\n",
    ")\n",
    "\n",
    "\n",
    "noise_model_path = \"/home/tharvey/work/synference/priv/data/JOF_psfmatched_asinh_noise_model.h5\"\n",
    "noise_models = load_unc_model_from_hdf5(noise_model_path)\n",
    "\n",
    "hst_filters = [\"F435W\", \"F606W\", \"F775W\", \"F814W\", \"F850LP\"]\n",
    "\n",
    "nm = {f\"JWST/NIRCam.{i}\": j for i, j in noise_models.items() if i not in hst_filters}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a205ec75",
   "metadata": {},
   "source": [
    "Smoothing params for asinh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68eef52d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print([-2.5 * np.log10(i.b.to_value(\"nJy\")) + 31.4 for i in nm.values()])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9d4e228",
   "metadata": {},
   "source": [
    "# Create feature arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba801ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "fa_input = dict(\n",
    "    extra_features=[\"redshift\"],\n",
    "    empirical_noise_models=nm,\n",
    "    include_errors_in_feature_array=True,\n",
    "    scatter_fluxes=True,\n",
    "    normed_flux_units=\"asinh\",\n",
    "    photometry_to_remove=[\n",
    "        \"CTIO/DECam.u\",\n",
    "        \"CTIO/DECam.g\",\n",
    "        \"CTIO/DECam.r\",\n",
    "        \"CTIO/DECam.i\",\n",
    "        \"CTIO/DECam.z\",\n",
    "        \"CTIO/DECam.Y\",\n",
    "        \"LSST/LSST.u\",\n",
    "        \"LSST/LSST.g\",\n",
    "        \"LSST/LSST.r\",\n",
    "        \"LSST/LSST.i\",\n",
    "        \"LSST/LSST.z\",\n",
    "        \"LSST/LSST.Y\",\n",
    "        \"PAN-STARRS/PS1.g\",\n",
    "        \"PAN-STARRS/PS1.r\",\n",
    "        \"PAN-STARRS/PS1.i\",\n",
    "        \"PAN-STARRS/PS1.w\",\n",
    "        \"PAN-STARRS/PS1.z\",\n",
    "        \"PAN-STARRS/PS1.y\",\n",
    "        \"Paranal/VISTA.Z\",\n",
    "        \"Paranal/VISTA.Y\",\n",
    "        \"Paranal/VISTA.J\",\n",
    "        \"Paranal/VISTA.H\",\n",
    "        \"Paranal/VISTA.Ks\",\n",
    "        \"Subaru/HSC.g\",\n",
    "        \"Subaru/HSC.r\",\n",
    "        \"Subaru/HSC.i\",\n",
    "        \"Subaru/HSC.z\",\n",
    "        \"Subaru/HSC.Y\",\n",
    "        \"CFHT/MegaCam.u\",\n",
    "        \"CFHT/MegaCam.g\",\n",
    "        \"CFHT/MegaCam.r\",\n",
    "        \"CFHT/MegaCam.i\",\n",
    "        \"CFHT/MegaCam.z\",\n",
    "        \"Euclid/VIS.vis\",\n",
    "        \"Euclid/NISP.Y\",\n",
    "        \"Euclid/NISP.J\",\n",
    "        \"Euclid/NISP.H\",\n",
    "        \"HST/ACS_WFC.F475W\",\n",
    "        \"HST/WFC3_IR.F105W\",\n",
    "        \"JWST/NIRCam.F070W\",\n",
    "        \"HST/WFC3_IR.F110W\",\n",
    "        \"HST/WFC3_IR.F125W\",\n",
    "        \"JWST/NIRCam.F140M\",\n",
    "        \"HST/WFC3_IR.F140W\",\n",
    "        \"HST/WFC3_IR.F160W\",\n",
    "        \"JWST/NIRCam.F360M\",\n",
    "        \"JWST/NIRCam.F430M\",\n",
    "        \"JWST/NIRCam.F460M\",\n",
    "        \"JWST/NIRCam.F480M\",\n",
    "        \"JWST/MIRI.F560W\",\n",
    "        \"JWST/MIRI.F770W\",\n",
    "        \"JWST/MIRI.F1000W\",\n",
    "        \"JWST/MIRI.F1130W\",\n",
    "        \"JWST/MIRI.F1280W\",\n",
    "        \"JWST/MIRI.F1500W\",\n",
    "        \"JWST/MIRI.F1800W\",\n",
    "        \"JWST/MIRI.F2100W\",\n",
    "        \"JWST/MIRI.F2550W\",\n",
    "        \"Spitzer/IRAC.I1\",\n",
    "        \"Spitzer/IRAC.I2\",\n",
    "        \"Spitzer/IRAC.I3\",\n",
    "        \"Spitzer/IRAC.I4\",\n",
    "        \"HST/ACS_WFC.F435W\",\n",
    "        \"HST/ACS_WFC.F606W\",\n",
    "        \"HST/ACS_WFC.F775W\",\n",
    "        \"HST/ACS_WFC.F814W\",\n",
    "        \"HST/ACS_WFC.F850LP\",\n",
    "    ],\n",
    ")\n",
    "\n",
    "\n",
    "fitter_pop3.create_feature_array_from_raw_photometry(\n",
    "    **fa_input, parameter_transformations={\"sfh\": np.log10}\n",
    ")\n",
    "fitter_bpass.create_feature_array_from_raw_photometry(\n",
    "    **fa_input, parameter_transformations={\"Av\": np.log10}\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "166c4d1e",
   "metadata": {},
   "source": [
    "# Plot feature arrays for comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08c52dee",
   "metadata": {},
   "outputs": [],
   "source": [
    "fitter_bpass.plot_histogram_feature_array();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7873ec38",
   "metadata": {},
   "outputs": [],
   "source": [
    "fitter_bpass.plot_histogram_parameter_array();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06725144",
   "metadata": {},
   "outputs": [],
   "source": [
    "fitter_pop3.plot_histogram_feature_array();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ca46d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "fitter_pop3.plot_histogram_parameter_array();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffd39087",
   "metadata": {},
   "source": [
    "# Train simformer models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "086e65be",
   "metadata": {},
   "outputs": [],
   "source": [
    "fitter_pop3.run_single_sbi(name_append=\"v1\", evaluate_model=False);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae3795dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "fitter_bpass.run_single_sbi(name_append=\"v1\", evaluate_model=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5824921",
   "metadata": {},
   "outputs": [],
   "source": [
    "import harmonic as hm\n",
    "\n",
    "\n",
    "class HarmonicEvidence:\n",
    "    \"\"\"Class to compute evidence using the targeted harmonic mean estimator.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"Class to compute evidence.\"\"\"\n",
    "        self.evidence = None\n",
    "\n",
    "    def from_nde(self, npe, nle, x, shape=(10000,), **sample_kwargs):\n",
    "        \"\"\"Estimate evidence from a posterior object.\n",
    "\n",
    "        Args:\n",
    "            npe (_type_): A Neural Posterior Estimator object with a\n",
    "                .sample() method.\n",
    "            nle (_type_): A Neural Likelihood Estimator object with a\n",
    "                .potential() method.\n",
    "            x (_type_): The input data to the posterior.\n",
    "            shape (tuple, optional): The shape of the samples to draw from the\n",
    "                posterior. Defaults to (10000,).\n",
    "            **sample_kwargs: Additional keyword arguments to pass to the\n",
    "                posterior's .sample() method.\n",
    "        \"\"\"\n",
    "        samples = npe.sample(shape, x, **sample_kwargs)\n",
    "        lnprob = nle.potential(samples, x)\n",
    "        self.from_samples(samples, lnprob)\n",
    "\n",
    "    def from_samples(self, samples, lnprob):\n",
    "        \"\"\"Estimate evidence from samples and log-probabilities.\n",
    "\n",
    "        # TODO: Add support for multiple chains\n",
    "\n",
    "        Args:\n",
    "            samples (array-like): An (N, D) array of samples,\n",
    "                where N is the number of samples, D is the dimensionality of\n",
    "                the samples.\n",
    "            lnprob (array-like): An (N,) array of log-likelihoods for each\n",
    "                sample\n",
    "        \"\"\"\n",
    "        if len(samples.shape) != 2:\n",
    "            raise ValueError(\"Samples shape must be (N, D).\")\n",
    "        if len(samples) != len(lnprob):\n",
    "            raise ValueError(\"Input samples and lnprob must be same length.\")\n",
    "        self._calculate_evidence(samples, lnprob)\n",
    "\n",
    "    def _calculate_evidence(self, samples, lnprob):\n",
    "        \"\"\"Internal function to calculate evidence with the targeted harmonic mean estimator.\n",
    "\n",
    "        Args:\n",
    "            samples (array-like): An (N, D) array of samples, where C is the\n",
    "                number of chains, N is the number of samples, and D is the\n",
    "                dimensionality of the samples.\n",
    "            lnprob (array-like): An (N,) array of proxy log-probabilities for\n",
    "                each sample.\n",
    "        \"\"\"\n",
    "        samples = np.asarray(samples)\n",
    "        lnprob = np.asarray(lnprob)\n",
    "        ndim = samples.shape[-1]\n",
    "\n",
    "        # Split samples into train/test\n",
    "        chains = hm.Chains(ndim)\n",
    "        chains.add_chain(samples, lnprob)\n",
    "        chains.split_into_blocks(100)\n",
    "        chains_train, chains_infer = hm.utils.split_data(chains, training_proportion=0.5)\n",
    "\n",
    "        # Select RealNVP Model\n",
    "        n_scaled_layers = 2\n",
    "        n_unscaled_layers = 4\n",
    "        temperature = 0.8\n",
    "\n",
    "        model = hm.model.RealNVPModel(\n",
    "            ndim,\n",
    "            n_scaled_layers=n_scaled_layers,\n",
    "            n_unscaled_layers=n_unscaled_layers,\n",
    "            standardize=True,\n",
    "            temperature=temperature,\n",
    "        )\n",
    "        epochs_num = 20\n",
    "        # Train model\n",
    "        model.fit(chains_train.samples, epochs=epochs_num, verbose=True)\n",
    "\n",
    "        # Save harmonic's evidence class\n",
    "        self.evidence = hm.Evidence(chains_infer.nchains, model)\n",
    "        self.evidence.add_chains(chains_infer)\n",
    "\n",
    "    def get_evidence(self):\n",
    "        \"\"\"Get the evidence.\"\"\"\n",
    "        if self.evidence is None:\n",
    "            raise ValueError(\"Evidence has not been computed.\")\n",
    "        return self.evidence.compute_evidence()\n",
    "\n",
    "    def get_ln_evidence(self):\n",
    "        \"\"\"Get the natural log of the evidence.\"\"\"\n",
    "        if self.evidence is None:\n",
    "            raise ValueError(\"Evidence has not been computed.\")\n",
    "        return self.evidence.compute_ln_evidence()\n",
    "\n",
    "    def get_bayes_factor(self, ev2):\n",
    "        \"\"\"Get the Bayes factor between this evidence and another.\"\"\"\n",
    "        if self.evidence is None:\n",
    "            raise ValueError(\"Evidence has not been computed.\")\n",
    "        if ev2.evidence is None:\n",
    "            raise ValueError(\"Evidence for model 2 has not been computed.\")\n",
    "        return hm.evidence.compute_bayes_factor(self.evidence, ev2.evidence)\n",
    "\n",
    "    def get_ln_bayes_factor(self, ev2):\n",
    "        \"\"\"Get the natural log of the Bayes factor between this evidence and another.\"\"\"\n",
    "        if self.evidence is None:\n",
    "            raise ValueError(\"Evidence has not been computed.\")\n",
    "        if ev2.evidence is None:\n",
    "            raise ValueError(\"Evidence for model 2 has not been computed.\")\n",
    "        return hm.evidence.compute_ln_bayes_factor(self.evidence, ev2.evidence)\n",
    "\n",
    "\n",
    "def get_bayes_factor(fitter1, fitter2, obs, shape=(10000,)):\n",
    "    \"\"\"Get the Bayes factor between two fitters for a given observation.\n",
    "\n",
    "    Args:\n",
    "        fitter1 (Simformer_Fitter): The first fitter.\n",
    "        fitter2 (Simformer_Fitter): The second fitter.\n",
    "        obs (array-like): The observation to compute the Bayes factor for.\n",
    "        shape (tuple, optional): The shape of the samples to draw from the\n",
    "            posterior. Defaults to (10000,).\n",
    "\n",
    "    Returns:\n",
    "        float: The Bayes factor (evidence ratio) between the two fitters.\n",
    "\n",
    "    \"\"\"\n",
    "    he1 = HarmonicEvidence()\n",
    "    he2 = HarmonicEvidence()\n",
    "\n",
    "    samples1 = fitter1.sample_posterior(obs)\n",
    "    print(samples1.shape)\n",
    "    lnprob1 = fitter1.log_prob(X_test=obs, theta=samples1)\n",
    "\n",
    "    print(lnprob1.shape)\n",
    "    he1.from_samples(samples1, lnprob1)\n",
    "\n",
    "    samples2 = fitter2.sample_posterior(obs)\n",
    "    lnprob2 = fitter2.log_prob(X_test=obs, theta=samples2)\n",
    "    he2.from_samples(samples2, lnprob2)\n",
    "\n",
    "    bf = he1.get_bayes_factor(he2)\n",
    "    return bf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "534413f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "fitter_bpass.posteriors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f79747dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick a random observation from each fitters validation set (._X_test)\n",
    "bpass_obs = fitter_bpass._X_test\n",
    "pop3_obs = fitter_pop3._X_test\n",
    "random_index_bpass = np.random.randint(0, len(bpass_obs))\n",
    "random_index_pop3 = np.random.randint(0, len(pop3_obs))\n",
    "\n",
    "bpass_obs_i = bpass_obs[random_index_bpass]\n",
    "pop3_obs_i = pop3_obs[random_index_pop3]\n",
    "\n",
    "pop3_obs_i, bpass_obs_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c244be6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take mock example data.\n",
    "print(\"Bayes factor (BPASS/PopIII) for BPASS-like observation:\")\n",
    "print(get_bayes_factor(fitter_bpass, fitter_pop3, bpass_obs_i, shape=(1000,)))\n",
    "\n",
    "print(\"Bayes factor (BPASS/PopIII) for PopIII-like observation:\")\n",
    "print(get_bayes_factor(fitter_bpass, fitter_pop3, pop3_obs_i, shape=(1000,)))"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
