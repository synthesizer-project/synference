{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f3129fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from copy import deepcopy\n",
    "\n",
    "import astropy.units as u\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from tqdm import tqdm\n",
    "from unyt import Jy, nJy\n",
    "\n",
    "# def smooth_sign(x, k=100.):\n",
    "#     return 2. * torch.sigmoid(k * x) - 1\n",
    "\n",
    "\n",
    "def parity_odd_power(x, alpha=2):\n",
    "    \"\"\"Applies the parity odd power transformation to the input tensor.\"\"\"\n",
    "    return x * (torch.abs(x) ** (alpha - 1))\n",
    "\n",
    "\n",
    "def leaky_parity_odd_power(x, alpha=2):\n",
    "    \"\"\"Applies the leaky parity odd power transformation to the input tensor.\"\"\"\n",
    "    return x + parity_odd_power(x, alpha)\n",
    "\n",
    "\n",
    "class POPExpLoss(nn.Module):\n",
    "    \"\"\"Exponential loss function for evidence network training.\"\"\"\n",
    "\n",
    "    def forward(self, model_pred, model_label):\n",
    "        \"\"\"Exponential loss function for evidence network training.\"\"\"\n",
    "        model_pred = leaky_parity_odd_power(model_pred, alpha=1)\n",
    "        model_pred = torch.clamp(model_pred, -50, 50)\n",
    "        loss_val = torch.exp((0.5 - model_label) * model_pred)\n",
    "        return torch.mean(loss_val)\n",
    "\n",
    "\n",
    "class ExpLoss(nn.Module):\n",
    "    \"\"\"Exponential loss function for evidence network training.\"\"\"\n",
    "\n",
    "    def forward(self, model_pred, model_label):\n",
    "        \"\"\"Exponential loss function for evidence network training.\"\"\"\n",
    "        loss_val = (0.5 - model_label) * model_pred\n",
    "        out = torch.logsumexp(loss_val, dim=0)\n",
    "        return out\n",
    "\n",
    "\n",
    "class EvidenceNetwork(nn.Module):\n",
    "    \"\"\"A deep neural network designed to approximate the log-Bayes factor.\n",
    "\n",
    "    The architecture consists of an optional embedding network, followed by an\n",
    "    initial fully-connected layer, a series of residual blocks, and a final\n",
    "    output layer. This structure allows the network to learn complex features from\n",
    "    the input data.\n",
    "\n",
    "    Args:\n",
    "        input_size (int): The dimension of the input features that are fed into the\n",
    "            first fully-connected layer. This should match the output dimension\n",
    "            of the `embedding_net`.\n",
    "        embedding_net (nn.Module, optional): A PyTorch module to preprocess or\n",
    "            embed the raw input data before it enters the main network. Defaults\n",
    "            to nn.Identity().\n",
    "        layer_width (int, optional): The number of neurons in the hidden layers.\n",
    "            Defaults to 16.\n",
    "        added_layers (int, optional): The number of residual blocks to include in\n",
    "            the network. Defaults to 3.\n",
    "        batch_norm_flag (int, optional): A flag to enable (1) or disable (0)\n",
    "            batch normalization. Defaults to 1.\n",
    "        alpha (int, optional): The exponent for the `leaky_parity_odd_power`\n",
    "            activation function applied to the output. Defaults to 2.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_size,\n",
    "        embedding_net=nn.Identity(),\n",
    "        layer_width=16,\n",
    "        added_layers=3,\n",
    "        batch_norm_flag=1,\n",
    "        alpha=2,\n",
    "    ):\n",
    "        \"\"\"Initializes the EvidenceNetwork with the specified architecture.\"\"\"\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.embedding_net = embedding_net\n",
    "        self.layer_width = layer_width\n",
    "        self.added_layers = added_layers\n",
    "        self.bn = batch_norm_flag\n",
    "        self.alpha = alpha\n",
    "\n",
    "        self.initial_layer = self.simple_layer(self.input_size, self.layer_width, 1)\n",
    "\n",
    "        self.hidden_layers = nn.ModuleList(\n",
    "            [self.simple_layer(self.layer_width, self.layer_width, 1) for _ in range(1)]\n",
    "        )\n",
    "\n",
    "        self.residual_layers = nn.ModuleList(\n",
    "            [\n",
    "                self.residual_layer(self.layer_width, self.layer_width, self.bn)\n",
    "                for _ in range(self.added_layers)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.post_residual = self.simple_layer(\n",
    "            self.layer_width, self.layer_width, batch_norm_flag=0\n",
    "        )\n",
    "        self.output_layer = nn.Linear(self.layer_width, 1)\n",
    "\n",
    "    def simple_layer(self, in_features, out_features, batch_norm_flag=1):\n",
    "        \"\"\"Creates a simple fully-connected layer with LeakyReLU activation.\"\"\"\n",
    "        layers = [nn.Linear(in_features, out_features), nn.LeakyReLU(0.1)]\n",
    "        if batch_norm_flag == 1:\n",
    "            layers.append(nn.BatchNorm1d(self.layer_width))\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def residual_layer(self, in_features, out_features, batch_norm_flag=1):\n",
    "        \"\"\"Creates a residual block with two fully-connected layers.\"\"\"\n",
    "        layers = [\n",
    "            self.simple_layer(in_features, out_features, batch_norm_flag),\n",
    "            self.simple_layer(out_features, out_features, batch_norm_flag),\n",
    "        ]\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward pass through the EvidenceNetwork.\"\"\"\n",
    "        # Embedding Network\n",
    "        x = self.embedding_net(x)\n",
    "\n",
    "        # Initial layer (resizing input to layer width)\n",
    "        x = self.initial_layer(x)\n",
    "\n",
    "        # Hidden layers\n",
    "        for layer in self.hidden_layers:\n",
    "            x = layer(x)\n",
    "\n",
    "        # Residual layers\n",
    "        for layer in self.residual_layers:\n",
    "            x = x + layer(x)\n",
    "\n",
    "        # Output layer\n",
    "        x = self.post_residual(x)\n",
    "        x = self.output_layer(x)\n",
    "        x = 0.1 * x + 0.001\n",
    "        x = leaky_parity_odd_power(x, alpha=self.alpha)\n",
    "        return x\n",
    "\n",
    "\n",
    "class K_EvidenceNetwork:\n",
    "    \"\"\"A runner class to train an EvidenceNetwork for approximating the Bayes Factor.\n",
    "\n",
    "    This class handles data preparation, training loops, validation, early stopping,\n",
    "    and model saving. It can optionally use a custom embedding network to process\n",
    "    complex data modalities before the main fully-connected layers.\n",
    "\n",
    "    Args:\n",
    "        embedding_net (nn.Module, optional): A PyTorch module to preprocess input\n",
    "            data. If None, an identity mapping is used. Defaults to None.\n",
    "        layer_width (int, optional): The number of neurons in the hidden layers of\n",
    "            the EvidenceNetwork. Defaults to 16.\n",
    "        added_layers (int, optional): The number of residual blocks in the\n",
    "            EvidenceNetwork. Defaults to 3.\n",
    "        batch_norm_flag (int, optional): A flag to enable (1) or disable (0)\n",
    "            batch normalization. Defaults to 1.\n",
    "        alpha (int, optional): The exponent for the leaky_parity_odd_power\n",
    "            activation function. Defaults to 2.\n",
    "        train_args (dict, optional): A dictionary of training hyperparameters to\n",
    "            override the defaults. Defaults to {}.\n",
    "        device (str, optional): The device to run training on (e.g., 'cpu' or\n",
    "            'cuda'). Defaults to 'cpu'.\n",
    "\n",
    "    Attributes:\n",
    "        model (EvidenceNetwork): The trained PyTorch model. This is None until\n",
    "            the `train` method is successfully called.\n",
    "        best_val (float): The best validation loss achieved during training.\n",
    "        loss_fn (nn.Module): The loss function instance used for training.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        embedding_net=None,\n",
    "        layer_width=16,\n",
    "        added_layers=3,\n",
    "        batch_norm_flag=1,\n",
    "        alpha=2,\n",
    "        train_args={},\n",
    "        device=\"cpu\",\n",
    "    ):\n",
    "        \"\"\"Initializes the K_EvidenceNetwork with specified architecture and training parameters.\"\"\"\n",
    "        self.embedding_net = embedding_net or nn.Identity()\n",
    "        self.layer_width = layer_width\n",
    "        self.added_layers = added_layers\n",
    "        self.batch_norm_flag = batch_norm_flag\n",
    "        self.alpha = alpha\n",
    "        self.train_args = dict(\n",
    "            training_batch_size=32,\n",
    "            learning_rate=1e-5,\n",
    "            stop_after_epochs=30,\n",
    "            clip_max_norm=5,\n",
    "            max_epochs=int(1e4),\n",
    "            validation_fraction=0.1,\n",
    "        )\n",
    "        self.train_args.update(train_args)\n",
    "        self.device = device\n",
    "\n",
    "        self.embedding_net.to(self.device)\n",
    "\n",
    "        self.loss_fn = ExpLoss()\n",
    "        self.best_val = float(\"inf\")\n",
    "        self.model = None\n",
    "\n",
    "    def _loss(self, model, theta, x):\n",
    "        \"\"\"Compute the loss function for a given model.\"\"\"\n",
    "        logK = model(x)\n",
    "        return self.loss_fn(logK, theta.view(-1, 1))\n",
    "\n",
    "    def _train_epoch(self, model, train_loader, val_loader, optimizer):\n",
    "        \"\"\"Train a single epoch of a neural network model.\"\"\"\n",
    "        model.train()\n",
    "        loss_train, count_train = [], 0\n",
    "        for x, theta in train_loader:\n",
    "            x, theta = x.to(self.device), theta.to(self.device)\n",
    "            optimizer.zero_grad()\n",
    "            loss = self._loss(model, theta, x)\n",
    "            loss.backward()\n",
    "\n",
    "            norm = nn.utils.clip_grad_norm_(model.parameters(), self.train_args[\"clip_max_norm\"])\n",
    "\n",
    "            if torch.isfinite(norm):\n",
    "                optimizer.step()\n",
    "            # Record\n",
    "            loss_train.append(loss.item() * len(theta))\n",
    "            count_train += len(theta)\n",
    "        loss_train = sum(loss_train) / count_train\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            loss_val, count_val = [], 0\n",
    "            for x, theta in val_loader:\n",
    "                x, theta = x.to(self.device), theta.to(self.device)\n",
    "                loss_val.append(self._loss(model, theta, x).item() * len(theta))\n",
    "                count_val += len(theta)\n",
    "        loss_val = sum(loss_val) / count_val\n",
    "        return loss_train, loss_val\n",
    "\n",
    "    def train(self, loader1, loader2, show_progress_bars=True):\n",
    "        \"\"\"Trains the evidence network.\n",
    "\n",
    "        Args:\n",
    "            loader1: A data loader object for the first model. Must have a\n",
    "                     `get_all_data()` method.\n",
    "            loader2: A data loader object for the second model.\n",
    "            show_progress_bars (bool): Whether to display tqdm progress bars.\n",
    "        \"\"\"\n",
    "        # Aggregate data from different models, and label them\n",
    "        # Assumes loaders have a 'get_all_data' method returning numpy arrays\n",
    "        x1 = loader1.get_all_data().astype(np.float32)\n",
    "        x2 = loader2.get_all_data().astype(np.float32)\n",
    "        x = np.concatenate([x1, x2], axis=0)\n",
    "        labels = np.concatenate([np.zeros(len(x1)), np.ones(len(x2))], axis=0)\n",
    "\n",
    "        # Train/Validation split\n",
    "        indices = np.random.permutation(len(x))\n",
    "        val_size = int(len(x) * self.train_args[\"validation_fraction\"])\n",
    "        val_indices, train_indices = indices[:val_size], indices[val_size:]\n",
    "        x_train, labels_train = x[train_indices], labels[train_indices]\n",
    "        x_val, labels_val = x[val_indices], labels[val_indices]\n",
    "\n",
    "        # Create data loaders\n",
    "        train_data = TensorDataset(\n",
    "            torch.tensor(x_train, dtype=torch.float32),\n",
    "            torch.tensor(labels_train, dtype=torch.float32),\n",
    "        )\n",
    "        val_data = TensorDataset(\n",
    "            torch.tensor(x_val, dtype=torch.float32), torch.tensor(labels_val, dtype=torch.float32)\n",
    "        )\n",
    "        train_loader = DataLoader(\n",
    "            train_data,\n",
    "            batch_size=self.train_args[\"training_batch_size\"],\n",
    "            shuffle=True,\n",
    "            drop_last=True,\n",
    "        )\n",
    "        val_loader = DataLoader(\n",
    "            val_data,\n",
    "            batch_size=self.train_args[\"training_batch_size\"],\n",
    "            shuffle=False,\n",
    "            drop_last=True,\n",
    "        )\n",
    "\n",
    "        # Determine the input dimension for the dense network\n",
    "        if not isinstance(self.embedding_net, nn.Identity):\n",
    "            with torch.no_grad():\n",
    "                # Use a single-item batch to find the output dimension\n",
    "                sample_input = next(iter(train_loader))[0].to(self.device)\n",
    "                embedded_output = self.embedding_net(sample_input)\n",
    "                ndim = embedded_output.shape[-1]\n",
    "        else:\n",
    "            # If no embedding net, the input dim is from the data itself\n",
    "            ndim = x_train.shape[-1]\n",
    "\n",
    "        # Create model, passing the embedding net\n",
    "        model = EvidenceNetwork(\n",
    "            input_size=ndim,\n",
    "            embedding_net=self.embedding_net,\n",
    "            layer_width=self.layer_width,\n",
    "            added_layers=self.added_layers,\n",
    "            batch_norm_flag=self.batch_norm_flag,\n",
    "            alpha=self.alpha,\n",
    "        )\n",
    "        model.to(self.device)\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=self.train_args[\"learning_rate\"])\n",
    "\n",
    "        # Train model\n",
    "        wait = 0\n",
    "        summary = {\"training_loss\": [], \"validation_loss\": []}\n",
    "        best_model_state = deepcopy(model.state_dict())\n",
    "\n",
    "        with tqdm(\n",
    "            iter(range(self.train_args[\"max_epochs\"])),\n",
    "            unit=\" epochs\",\n",
    "            disable=not show_progress_bars,\n",
    "        ) as tq:\n",
    "            for epoch in tq:\n",
    "                loss_train, loss_val = self._train_epoch(\n",
    "                    model=model,\n",
    "                    train_loader=train_loader,\n",
    "                    val_loader=val_loader,\n",
    "                    optimizer=optimizer,\n",
    "                )\n",
    "                if show_progress_bars:\n",
    "                    tq.set_postfix(loss_train=f\"{loss_train:.4f}\", loss_val=f\"{loss_val:.4f}\")\n",
    "\n",
    "                summary[\"training_loss\"].append(loss_train)\n",
    "                summary[\"validation_loss\"].append(loss_val)\n",
    "\n",
    "                if loss_val < self.best_val:\n",
    "                    self.best_val = loss_val\n",
    "                    best_model_state = deepcopy(model.state_dict())\n",
    "                    wait = 0\n",
    "                elif wait >= self.train_args[\"stop_after_epochs\"]:\n",
    "                    logging.info(f\"Stopping early after {epoch} epochs.\")\n",
    "                    break\n",
    "                else:\n",
    "                    wait += 1\n",
    "            else:\n",
    "                logging.warning(\n",
    "                    f\"Training did not converge in {self.train_args['max_epochs']} epochs.\"\n",
    "                )\n",
    "\n",
    "        summary[\"best_validation_loss\"] = self.best_val\n",
    "        summary[\"epochs_trained\"] = epoch + 1\n",
    "        self.model = model\n",
    "        self.model.load_state_dict(best_model_state)\n",
    "        return summary\n",
    "\n",
    "    def predict(self, x):\n",
    "        \"\"\"Predict the log-Bayes Ratio for a given input.\"\"\"\n",
    "        if self.model is None:\n",
    "            raise ValueError(\"Model has not been trained yet. Please call .train() first.\")\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            x_tensor = torch.from_numpy(np.atleast_2d(x).astype(np.float32)).to(self.device)\n",
    "            return self.model(x_tensor).cpu().numpy()\n",
    "\n",
    "\n",
    "def plot_histogram(logK_bpass, logK_pop3):\n",
    "    \"\"\"Plot histogram of log Bayes factors for two models with Jeffreys scale annotations.\"\"\"\n",
    "    plt.figure(figsize=(10, 6))\n",
    "\n",
    "    jeffries_logK = {\n",
    "        (-100, -2): \"deciseivly against\",\n",
    "        (-2, -1.5): \"very strongly against\",\n",
    "        (-1.5, -1): \"strongly against\",\n",
    "        (-1, -0.5): \"moderately against\",\n",
    "        (-0.5, 0.5): \"No strong evidence\",\n",
    "        (0.5, 1): \"moderately for\",\n",
    "        (1, 1.5): \"strongly for\",\n",
    "        (1.5, 2): \"very strongly for\",\n",
    "        (2, 100): \"decisively for\",\n",
    "    }\n",
    "\n",
    "    # Log-spaced bins from -100 to 100\n",
    "    bins = np.concatenate((-np.logspace(0, 2, 20)[::-1], np.logspace(0, 2, 20)))\n",
    "    plt.hist(logK_bpass, bins=bins, alpha=0.5, label=\"BPASS\", color=\"C0\")\n",
    "    plt.hist(logK_pop3, bins=bins, alpha=0.5, label=\"Pop III\", color=\"C1\")\n",
    "    plt.axvline(0, color=\"k\", linestyle=\"--\", alpha=0.4)\n",
    "    plt.xlabel(\"log K (Pop III / BPASS) \")\n",
    "    plt.ylabel(\"Number of samples\")\n",
    "\n",
    "    # add shaded regions with text (white text on dark background)\n",
    "    for pos, ((x0, x1), label) in enumerate(jeffries_logK.items()):\n",
    "        plt.axvspan(x0, x1, color=\"gray\", alpha=0.2 if pos % 2 == 0 else 0.1)\n",
    "        plt.text(\n",
    "            (x0 + x1) / 2,\n",
    "            plt.ylim()[1] * 0.9,\n",
    "            label,\n",
    "            color=\"black\" if pos % 2 == 0 else \"dimgray\",\n",
    "            ha=\"center\",\n",
    "            va=\"top\",\n",
    "            fontsize=8,\n",
    "            rotation=90,\n",
    "        )\n",
    "\n",
    "    # Add a symlog scale on the x-axis\n",
    "    plt.xscale(\"symlog\", linthresh=3)\n",
    "\n",
    "    # Label either end of the x-axis with the corresponding model\n",
    "    plt.text(-90, plt.ylim()[1] * 0.94, \"BPASS\", color=\"C0\", fontsize=12, ha=\"left\")\n",
    "    plt.text(90, plt.ylim()[1] * 0.94, \"PopIII\", color=\"C1\", fontsize=12, ha=\"right\")\n",
    "\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb742b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_histogram(logK_X, logK_Y, Y_label=\"Model Y\", X_label=\"Model X\"):\n",
    "    \"\"\"Plot histogram of log Bayes factors for two models with Jeffreys scale annotations.\"\"\"\n",
    "    plt.figure(figsize=(10, 6))\n",
    "\n",
    "    jeffries_logK = {\n",
    "        (-100, -2): \"decisively against\",\n",
    "        (-2, -1.5): \"very strongly against\",\n",
    "        (-1.5, -1): \"strongly against\",\n",
    "        (-1, -0.5): \"moderately against\",\n",
    "        (-0.5, 0.5): \"No strong evidence\",\n",
    "        (0.5, 1): \"moderately for\",\n",
    "        (1, 1.5): \"strongly for\",\n",
    "        (1.5, 2): \"very strongly for\",\n",
    "        (2, 100): \"decisively for\",\n",
    "    }\n",
    "\n",
    "    # Jeffries bins (with more detail between -100 and 2 and 2 and 100)\n",
    "    bins = np.concatenate(\n",
    "        [\n",
    "            -np.logspace(0.31, 2, 20)[::-1],\n",
    "            np.linspace(-2, 2, 20),\n",
    "            np.logspace(0.31, 2, 20),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    plt.hist(logK_X, bins=bins, alpha=0.5, label=f\"True {X_label} Samples\", color=\"#7D8BE0\")\n",
    "    plt.hist(logK_Y, bins=bins, alpha=0.5, label=f\"True {Y_label} Samples\", color=\"#EA7D70\")\n",
    "    plt.axvline(0, color=\"k\", linestyle=\"--\", alpha=0.4)\n",
    "    plt.xlabel(f\"log K ( {Y_label} / {X_label}) \")\n",
    "    plt.ylabel(\"Number of samples\")\n",
    "\n",
    "    # add shaded regions with text (white text on dark background)\n",
    "    for pos, ((x0, x1), label) in enumerate(jeffries_logK.items()):\n",
    "        plt.axvspan(x0, x1, color=\"gray\", alpha=0.2 if pos % 2 == 0 else 0.1)\n",
    "        plt.text(\n",
    "            (x0 + x1) / 2,\n",
    "            plt.ylim()[1] * 0.9,\n",
    "            label,\n",
    "            color=\"black\" if pos % 2 == 0 else \"dimgray\",\n",
    "            ha=\"center\",\n",
    "            va=\"top\",\n",
    "            fontsize=8,\n",
    "            rotation=90,\n",
    "        )\n",
    "\n",
    "    # Add a symlog scale on the x-axis\n",
    "    plt.xscale(\"symlog\", linthresh=3)\n",
    "\n",
    "    # Label either end of the x-axis with the corresponding model\n",
    "    plt.text(\n",
    "        -90,\n",
    "        plt.ylim()[1] * 0.94,\n",
    "        f\"Evidence for {X_label}\",\n",
    "        color=\"#7D8BE0\",\n",
    "        fontsize=12,\n",
    "        ha=\"left\",\n",
    "    )\n",
    "    plt.text(\n",
    "        90,\n",
    "        plt.ylim()[1] * 0.94,\n",
    "        f\"Evidence for {Y_label}\",\n",
    "        color=\"#EA7D70\",\n",
    "        fontsize=12,\n",
    "        ha=\"right\",\n",
    "    )\n",
    "\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "868cb645",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from synference import (\n",
    "    SBI_Fitter,\n",
    "    create_uncertainty_models_from_EPOCHS_cat,\n",
    "    load_unc_model_from_hdf5,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f3e703c",
   "metadata": {},
   "outputs": [],
   "source": [
    "bpass_path = \"/home/tharvey/work/synference/libraries/grid_BPASS_Chab_DenseBasis_SFH_0.01_z_14_logN_5.0_Calzetti_v3_multinode.hdf5\"  # noqa: E501\n",
    "\n",
    "fitter_bpass = SBI_Fitter.init_from_hdf5(\n",
    "    \"BPASS_Chab_DenseBasis_SFH_0.01_z_14_logN_5.0_Calzetti_v3_multinode\", bpass_path\n",
    ")\n",
    "\n",
    "grid_pop3_path = (\n",
    "    \"/home/tharvey/work/synference/libraries/grid_Yggdrasil_Chab_Burst_SFH_0.001_z_14_logN_4.4_v1.hdf5\"  # noqa: E501\n",
    ")\n",
    "\n",
    "fitter_pop3 = SBI_Fitter.init_from_hdf5(\n",
    "    \"Yggdrasil_Chab_Burst_SFH_0.001_z_14_logN_4.4_v1\", grid_pop3_path\n",
    ")\n",
    "\n",
    "\n",
    "noise_model_path = \"/home/tharvey/work/synference/priv/data/JOF_psfmatched_asinh_noise_model.h5\"\n",
    "noise_models = load_unc_model_from_hdf5(noise_model_path)\n",
    "\n",
    "hst_filters = [\"F435W\", \"F606W\", \"F775W\", \"F814W\", \"F850LP\"]\n",
    "\n",
    "nm = {f\"JWST/NIRCam.{i}\": j for i, j in noise_models.items() if i not in hst_filters}\n",
    "\n",
    "softening_parameter = (35 * u.ABmag).to_value(\"Jy\") * Jy\n",
    "\n",
    "\n",
    "depths = {  # per pixel at 5-sigma\n",
    "    \"HST/ACS_WFC.F435W\": 0.394 * nJy,\n",
    "    \"HST/ACS_WFC.F606W\": 0.611 * nJy,\n",
    "    \"HST/ACS_WFC.F775W\": 0.376 * nJy,\n",
    "    \"HST/ACS_WFC.F814W\": 1.388 * nJy,\n",
    "    \"HST/ACS_WFC.F850LP\": 0.724 * nJy,\n",
    "    \"JWST/NIRCam.F090W\": 0.601 * nJy,\n",
    "    \"JWST/NIRCam.F115W\": 0.496 * nJy,\n",
    "    \"JWST/NIRCam.F150W\": 0.489 * nJy,\n",
    "    \"JWST/NIRCam.F182M\": 1.361 * nJy,\n",
    "    \"JWST/NIRCam.F200W\": 0.509 * nJy,\n",
    "    \"JWST/NIRCam.F210M\": 0.987 * nJy,\n",
    "    \"JWST/NIRCam.F277W\": 0.367 * nJy,\n",
    "    \"JWST/NIRCam.F335M\": 0.616 * nJy,\n",
    "    \"JWST/NIRCam.F356W\": 0.416 * nJy,\n",
    "    \"JWST/NIRCam.F410M\": 0.547 * nJy,\n",
    "    \"JWST/NIRCam.F430M\": 1.327 * nJy,\n",
    "    \"JWST/NIRCam.F444W\": 0.472 * nJy,\n",
    "    \"JWST/NIRCam.F460M\": 1.813 * nJy,\n",
    "    \"JWST/NIRCam.F480M\": 1.351 * nJy,\n",
    "}\n",
    "\n",
    "fa_input = dict(\n",
    "    extra_features=[\"redshift\"],\n",
    "    # empirical_noise_models=nm,\n",
    "    include_errors_in_feature_array=True,\n",
    "    scatter_fluxes=True,\n",
    "    normed_flux_units=\"asinh\",\n",
    "    depths=depths,\n",
    "    photometry_to_remove=[\n",
    "        \"CTIO/DECam.u\",\n",
    "        \"CTIO/DECam.g\",\n",
    "        \"CTIO/DECam.r\",\n",
    "        \"CTIO/DECam.i\",\n",
    "        \"CTIO/DECam.z\",\n",
    "        \"CTIO/DECam.Y\",\n",
    "        \"LSST/LSST.u\",\n",
    "        \"LSST/LSST.g\",\n",
    "        \"LSST/LSST.r\",\n",
    "        \"LSST/LSST.i\",\n",
    "        \"LSST/LSST.z\",\n",
    "        \"LSST/LSST.Y\",\n",
    "        \"PAN-STARRS/PS1.g\",\n",
    "        \"PAN-STARRS/PS1.r\",\n",
    "        \"PAN-STARRS/PS1.i\",\n",
    "        \"PAN-STARRS/PS1.w\",\n",
    "        \"PAN-STARRS/PS1.z\",\n",
    "        \"PAN-STARRS/PS1.y\",\n",
    "        \"Paranal/VISTA.Z\",\n",
    "        \"Paranal/VISTA.Y\",\n",
    "        \"Paranal/VISTA.J\",\n",
    "        \"Paranal/VISTA.H\",\n",
    "        \"Paranal/VISTA.Ks\",\n",
    "        \"Subaru/HSC.g\",\n",
    "        \"Subaru/HSC.r\",\n",
    "        \"Subaru/HSC.i\",\n",
    "        \"Subaru/HSC.z\",\n",
    "        \"Subaru/HSC.Y\",\n",
    "        \"CFHT/MegaCam.u\",\n",
    "        \"CFHT/MegaCam.g\",\n",
    "        \"CFHT/MegaCam.r\",\n",
    "        \"CFHT/MegaCam.i\",\n",
    "        \"CFHT/MegaCam.z\",\n",
    "        \"Euclid/VIS.vis\",\n",
    "        \"Euclid/NISP.Y\",\n",
    "        \"Euclid/NISP.J\",\n",
    "        \"Euclid/NISP.H\",\n",
    "        \"HST/ACS_WFC.F475W\",\n",
    "        \"HST/WFC3_IR.F105W\",\n",
    "        \"JWST/NIRCam.F070W\",\n",
    "        \"HST/WFC3_IR.F110W\",\n",
    "        \"HST/WFC3_IR.F125W\",\n",
    "        \"JWST/NIRCam.F140M\",\n",
    "        \"HST/WFC3_IR.F140W\",\n",
    "        \"HST/WFC3_IR.F160W\",\n",
    "        \"JWST/NIRCam.F162M\",\n",
    "        \"JWST/NIRCam.F250M\",\n",
    "        \"JWST/NIRCam.F300M\",\n",
    "        \"JWST/NIRCam.F360M\",\n",
    "        \"JWST/NIRCam.F430M\",\n",
    "        \"JWST/NIRCam.F460M\",\n",
    "        \"JWST/NIRCam.F480M\",\n",
    "        \"JWST/MIRI.F560W\",\n",
    "        \"JWST/MIRI.F770W\",\n",
    "        \"JWST/MIRI.F1000W\",\n",
    "        \"JWST/MIRI.F1130W\",\n",
    "        \"JWST/MIRI.F1280W\",\n",
    "        \"JWST/MIRI.F1500W\",\n",
    "        \"JWST/MIRI.F1800W\",\n",
    "        \"JWST/MIRI.F2100W\",\n",
    "        \"JWST/MIRI.F2550W\",\n",
    "        \"Spitzer/IRAC.I1\",\n",
    "        \"Spitzer/IRAC.I2\",\n",
    "        \"Spitzer/IRAC.I3\",\n",
    "        \"Spitzer/IRAC.I4\",\n",
    "        \"HST/ACS_WFC.F435W\",\n",
    "        \"HST/ACS_WFC.F606W\",\n",
    "        \"HST/ACS_WFC.F775W\",\n",
    "        \"HST/ACS_WFC.F814W\",\n",
    "        \"HST/ACS_WFC.F850LP\",\n",
    "    ],\n",
    "    asinh_softening_parameters=softening_parameter,\n",
    ")\n",
    "\n",
    "fitter_bpass.create_feature_array_from_raw_photometry(\n",
    "    **fa_input, parameter_transformations={\"Av\": np.log10}\n",
    ")\n",
    "\n",
    "fitter_pop3.create_feature_array_from_raw_photometry(\n",
    "    **fa_input,\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88494781",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_idx, test_idx = fitter_bpass.split_dataset(random_seed=42)\n",
    "\n",
    "# Make loaders\n",
    "from ili.dataloaders import NumpyLoader\n",
    "\n",
    "train_loader_bpass = NumpyLoader(\n",
    "    fitter_bpass.feature_array[train_idx].astype(np.float32),\n",
    "    fitter_bpass.fitted_parameter_array[train_idx].astype(np.float32),\n",
    ")\n",
    "test_loader_bpass = NumpyLoader(\n",
    "    fitter_bpass.feature_array[test_idx].astype(np.float32),\n",
    "    fitter_bpass.fitted_parameter_array[test_idx].astype(np.float32),\n",
    ")\n",
    "\n",
    "\n",
    "train_idx, test_idx = fitter_pop3.split_dataset(random_seed=42)\n",
    "\n",
    "# Make loaders\n",
    "\n",
    "train_loader_pop3 = NumpyLoader(\n",
    "    fitter_pop3.feature_array[train_idx].astype(np.float32),\n",
    "    fitter_pop3.fitted_parameter_array[train_idx].astype(np.float32),\n",
    ")\n",
    "test_loader_pop3 = NumpyLoader(\n",
    "    fitter_pop3.feature_array[test_idx].astype(np.float32),\n",
    "    fitter_pop3.fitted_parameter_array[test_idx].astype(np.float32),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12aa075d",
   "metadata": {},
   "outputs": [],
   "source": [
    "network = K_EvidenceNetwork()\n",
    "summary = network.train(train_loader_bpass, train_loader_pop3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33499360",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick 10 random samples from each test set\n",
    "np.random.seed(42)\n",
    "\n",
    "# Do all of both test sets\n",
    "\n",
    "x_bpass = test_loader_bpass.get_all_data()\n",
    "x_pop3 = test_loader_pop3.get_all_data()\n",
    "\n",
    "logK_bpass = network.predict(x_bpass)\n",
    "logK_pop3 = network.predict(x_pop3)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Clip to 100, -100\n",
    "logK_bpass = np.clip(logK_bpass, -100, 100)\n",
    "logK_pop3 = np.clip(logK_pop3, -100, 100)\n",
    "\n",
    "\n",
    "plot_histogram(logK_bpass, logK_pop3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e7d0777",
   "metadata": {},
   "source": [
    "#Â Train and test a model comparison for all Pop III models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23f1aaed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try all Yggdrasil models\n",
    "grids = [\n",
    "    \"grid_yggdrasil-1.3.3-PopIII_kroupa-0.1,100_Burst_SFH_0.001_z_14_logN_4.4_v1\",\n",
    "    \"grid_yggdrasil-1.3.3-PopIII_salpeter-10,1,500_Burst_SFH_0.001_z_14_logN_4.4_v1\",\n",
    "    \"grid_yggdrasil-1.3.3-PopIII_salpeter-50,500_Burst_SFH_0.001_z_14_logN_4.4_v1\",\n",
    "    \"grid_yggdrasil-1.3.3-POPIII-fcov_0.5_kroupa-0.1,100_Burst_SFH_0.001_z_14_logN_4.4_v1\",\n",
    "    \"grid_yggdrasil-1.3.3-POPIII-fcov_0.5_salpeter-10,1,500_Burst_SFH_0.001_z_14_logN_4.4_v1\",\n",
    "    \"grid_yggdrasil-1.3.3-POPIII-fcov_0.5_salpeter-50,500_Burst_SFH_0.001_z_14_logN_4.4_v1\",\n",
    "    \"grid_yggdrasil-1.3.3-POPIII-fcov_1_kroupa-0.1,100_Burst_SFH_0.001_z_14_logN_4.4_v1\",\n",
    "    \"grid_yggdrasil-1.3.3-POPIII-fcov_1_salpeter-10,1,500_Burst_SFH_0.001_z_14_logN_4.4_v1\",\n",
    "    \"grid_yggdrasil-1.3.3-POPIII-fcov_1_salpeter-50,500_Burst_SFH_0.001_z_14_logN_4.4_v1\",\n",
    "]\n",
    "\n",
    "for grid in grids:\n",
    "    print(f\"Processing grid: {grid}\")\n",
    "    fitter_pop3 = SBI_Fitter.init_from_hdf5(\n",
    "        grid,\n",
    "        f\"{grid}.hdf5\",\n",
    "    )\n",
    "    fitter_pop3.create_feature_array_from_raw_photometry(\n",
    "        **fa_input, parameter_transformations={\"sfh\": np.log10}\n",
    "    )\n",
    "\n",
    "    train_idx, test_idx = fitter_pop3.split_dataset(random_seed=42)\n",
    "    train_loader_pop3 = NumpyLoader(\n",
    "        fitter_pop3.feature_array[train_idx].astype(np.float32),\n",
    "        fitter_pop3.fitted_parameter_array[train_idx].astype(np.float32),\n",
    "    )\n",
    "\n",
    "    test_loader_pop3 = NumpyLoader(\n",
    "        fitter_pop3.feature_array[test_idx].astype(np.float32),\n",
    "        fitter_pop3.fitted_parameter_array[test_idx].astype(np.float32),\n",
    "    )\n",
    "\n",
    "    network = K_EvidenceNetwork()\n",
    "    summary = network.train(train_loader_bpass, train_loader_pop3)\n",
    "    x_pop3 = test_loader_pop3.get_all_data()\n",
    "    x_bpass = test_loader_bpass.get_all_data()\n",
    "    logK_pop3 = network.predict(x_pop3)\n",
    "    logK_bpass = network.predict(x_bpass)\n",
    "\n",
    "    logK_pop3 = np.clip(logK_pop3, -100, 100)\n",
    "    logK_bpass = np.clip(logK_bpass, -100, 100)\n",
    "    plot_histogram(logK_bpass, logK_pop3)\n",
    "    plt.title(grid)\n",
    "    plt.savefig(f\"/home/tharvey/work/synference/priv/pop3_bpass_logK_{grid}.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abdf4d7f",
   "metadata": {},
   "source": [
    "# FSPS vs BPASS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44bee923",
   "metadata": {},
   "outputs": [],
   "source": [
    "bpass_path = \"/home/tharvey/work/synference/libraries/grid_BPASS_Chab_DenseBasis_SFH_0.01_z_14_logN_5.0_Calzetti_v3_multinode.hdf5\"  # noqa: E501\n",
    "\n",
    "fitter_bpass = SBI_Fitter.init_from_hdf5(\n",
    "    \"BPASS_Chab_DenseBasis_SFH_0.01_z_14_logN_5.0_Calzetti_v3_multinode\", bpass_path\n",
    ")\n",
    "\n",
    "grid_fsps = \"/home/tharvey/work/synference/libraries/grid_FSPS_Chab_DenseBasis_SFH_0.01_z_14_logN_6.0_Calzetti_v4_multinode.hdf5\"  # noqa: E501\n",
    "\n",
    "fitter_fsps = SBI_Fitter.init_from_hdf5(\n",
    "    \"FSPS_Chab_DenseBasis_SFH_0.01_z_14_logN_6.0_Calzetti_v4_multinode\", grid_fsps\n",
    ")\n",
    "\n",
    "bands = [\n",
    "    \"F435W\",\n",
    "    \"F606W\",\n",
    "    \"F775W\",\n",
    "    \"F814W\",\n",
    "    \"F850LP\",\n",
    "    \"F090W\",\n",
    "    \"F115W\",\n",
    "    \"F150W\",\n",
    "    \"F200W\",\n",
    "    \"F277W\",\n",
    "    \"F335M\",\n",
    "    \"F356W\",\n",
    "    \"F410M\",\n",
    "    \"F444W\",\n",
    "]\n",
    "\n",
    "hst_bands = [\"F435W\", \"F606W\", \"F775W\", \"F814W\", \"F850LP\"]\n",
    "new_band_names = [\n",
    "    (f\"HST/ACS_WFC.{band.upper()}\" if band in hst_bands else f\"JWST/NIRCam.{band.upper()}\")\n",
    "    for band in bands\n",
    "]\n",
    "data_err_file = \"/home/tharvey/work/JADES-DR3-GS_MASTER_Sel-F277W+F356W+F444W_v13.fits\"\n",
    "\n",
    "\n",
    "empirical_noise_models = create_uncertainty_models_from_EPOCHS_cat(\n",
    "    data_err_file,\n",
    "    bands,\n",
    "    new_band_names,\n",
    "    plot=False,\n",
    "    hdu=\"OBJECTS\",\n",
    "    save=False,\n",
    "    min_flux_error=0,\n",
    "    model_class=\"asinh\",\n",
    ")\n",
    "\n",
    "\n",
    "fa_input = dict(\n",
    "    extra_features=[\"redshift\"],\n",
    "    empirical_noise_models=empirical_noise_models,\n",
    "    include_errors_in_feature_array=True,\n",
    "    scatter_fluxes=True,\n",
    "    normed_flux_units=\"asinh\",\n",
    "    photometry_to_remove=[\n",
    "        \"CTIO/DECam.u\",\n",
    "        \"CTIO/DECam.g\",\n",
    "        \"CTIO/DECam.r\",\n",
    "        \"CTIO/DECam.i\",\n",
    "        \"CTIO/DECam.z\",\n",
    "        \"CTIO/DECam.Y\",\n",
    "        \"LSST/LSST.u\",\n",
    "        \"LSST/LSST.g\",\n",
    "        \"LSST/LSST.r\",\n",
    "        \"LSST/LSST.i\",\n",
    "        \"LSST/LSST.z\",\n",
    "        \"LSST/LSST.Y\",\n",
    "        \"PAN-STARRS/PS1.g\",\n",
    "        \"PAN-STARRS/PS1.r\",\n",
    "        \"PAN-STARRS/PS1.i\",\n",
    "        \"PAN-STARRS/PS1.w\",\n",
    "        \"PAN-STARRS/PS1.z\",\n",
    "        \"PAN-STARRS/PS1.y\",\n",
    "        \"Paranal/VISTA.Z\",\n",
    "        \"Paranal/VISTA.Y\",\n",
    "        \"Paranal/VISTA.J\",\n",
    "        \"Paranal/VISTA.H\",\n",
    "        \"Paranal/VISTA.Ks\",\n",
    "        \"Subaru/HSC.g\",\n",
    "        \"Subaru/HSC.r\",\n",
    "        \"Subaru/HSC.i\",\n",
    "        \"Subaru/HSC.z\",\n",
    "        \"Subaru/HSC.Y\",\n",
    "        \"CFHT/MegaCam.u\",\n",
    "        \"CFHT/MegaCam.g\",\n",
    "        \"CFHT/MegaCam.r\",\n",
    "        \"CFHT/MegaCam.i\",\n",
    "        \"CFHT/MegaCam.z\",\n",
    "        \"Euclid/VIS.vis\",\n",
    "        \"Euclid/NISP.Y\",\n",
    "        \"Euclid/NISP.J\",\n",
    "        \"Euclid/NISP.H\",\n",
    "        \"HST/ACS_WFC.F475W\",\n",
    "        \"HST/WFC3_IR.F105W\",\n",
    "        \"JWST/NIRCam.F070W\",\n",
    "        \"HST/WFC3_IR.F110W\",\n",
    "        \"HST/WFC3_IR.F125W\",\n",
    "        \"JWST/NIRCam.F140M\",\n",
    "        \"HST/WFC3_IR.F140W\",\n",
    "        \"HST/WFC3_IR.F160W\",\n",
    "        \"JWST/NIRCam.F162M\",\n",
    "        \"JWST/NIRCam.F182M\",\n",
    "        \"JWST/NIRCam.F210M\",\n",
    "        \"JWST/NIRCam.F250M\",\n",
    "        \"JWST/NIRCam.F300M\",\n",
    "        \"JWST/NIRCam.F360M\",\n",
    "        \"JWST/NIRCam.F430M\",\n",
    "        \"JWST/NIRCam.F460M\",\n",
    "        \"JWST/NIRCam.F480M\",\n",
    "        \"JWST/MIRI.F560W\",\n",
    "        \"JWST/MIRI.F770W\",\n",
    "        \"JWST/MIRI.F1000W\",\n",
    "        \"JWST/MIRI.F1130W\",\n",
    "        \"JWST/MIRI.F1280W\",\n",
    "        \"JWST/MIRI.F1500W\",\n",
    "        \"JWST/MIRI.F1800W\",\n",
    "        \"JWST/MIRI.F2100W\",\n",
    "        \"JWST/MIRI.F2550W\",\n",
    "        \"Spitzer/IRAC.I1\",\n",
    "        \"Spitzer/IRAC.I2\",\n",
    "        \"Spitzer/IRAC.I3\",\n",
    "        \"Spitzer/IRAC.I4\",\n",
    "    ],\n",
    ")\n",
    "\n",
    "fitter_bpass.create_feature_array_from_raw_photometry(\n",
    "    **fa_input,\n",
    ")\n",
    "\n",
    "fitter_fsps.create_feature_array_from_raw_photometry(\n",
    "    **fa_input,\n",
    "    max_rows=100_000,\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b3e05bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_idx, test_idx = fitter_bpass.split_dataset(random_seed=42)\n",
    "\n",
    "# Make loaders\n",
    "from ili.dataloaders import NumpyLoader\n",
    "\n",
    "train_loader_bpass = NumpyLoader(\n",
    "    fitter_bpass.feature_array[train_idx].astype(np.float32),\n",
    "    fitter_bpass.fitted_parameter_array[train_idx].astype(np.float32),\n",
    ")\n",
    "test_loader_bpass = NumpyLoader(\n",
    "    fitter_bpass.feature_array[test_idx].astype(np.float32),\n",
    "    fitter_bpass.fitted_parameter_array[test_idx].astype(np.float32),\n",
    ")\n",
    "\n",
    "\n",
    "train_idx, test_idx = fitter_fsps.split_dataset(random_seed=42)\n",
    "\n",
    "# Make loaders\n",
    "\n",
    "train_loader_fsps = NumpyLoader(\n",
    "    fitter_fsps.feature_array[train_idx].astype(np.float32),\n",
    "    fitter_fsps.fitted_parameter_array[train_idx].astype(np.float32),\n",
    ")\n",
    "test_loader_fsps = NumpyLoader(\n",
    "    fitter_fsps.feature_array[test_idx].astype(np.float32),\n",
    "    fitter_fsps.fitted_parameter_array[test_idx].astype(np.float32),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6b674fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "network = K_EvidenceNetwork(device=\"cuda\", layer_width=32, added_layers=5)\n",
    "summary = network.train(train_loader_bpass, train_loader_fsps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3933e8e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick 10 random samples from each test set\n",
    "np.random.seed(42)\n",
    "\n",
    "# Do all of both test sets\n",
    "\n",
    "x_bpass = test_loader_bpass.get_all_data()\n",
    "x_fsps = test_loader_fsps.get_all_data()\n",
    "\n",
    "logK_bpass = network.predict(x_bpass)\n",
    "logK_fsps = network.predict(x_fsps)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Clip to 100, -100\n",
    "logK_bpass = np.clip(logK_bpass, -100, 100)\n",
    "logK_fsps = np.clip(logK_fsps, -100, 100)\n",
    "\n",
    "\n",
    "plot_histogram(logK_bpass, logK_fsps, X_label=\"BPASS\", Y_label=\"FSPS\")\n",
    "plt.yscale(\"log\")"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
