{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2291bddd",
   "metadata": {},
   "source": [
    "### The Simformer\n",
    "\n",
    "[Gloeckler et al. 2024](https://arxiv.org/abs/2404.09636) introduced the Simformer, for 'all in one simulation based inference'.\n",
    "\n",
    "They use a novel probablistic diffusion model with a transformer architecture which learns the full joint distribution of parameters and data, allowing for fast, amortized Bayesian inference, without specifying beforehand which parameters are of interest. This makes the simformer approach particularly well suited to missing data, as the use of an attention mechanism allows the model sample from an arbitrary conditional distribution excluding any missing data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12ddb0a9",
   "metadata": {},
   "source": [
    "The Simformer is currently implemented in two ways. The first way, is a seperate class called ```Simformer_Fitter```, which requires the user to install a [fork of the original simformer repo](https://github.com/tHarvey303/simformer/), which requires quite specific versions of CUDA, PyTorch and jax to work. \n",
    "\n",
    "The second way uses the new simformer implementation in the sbi package, which is currently only available in a pull request, but should be merged into the main branch soon. For now we will deal with this implementation, as it is much easier to install and use. There are examples of using the original approach in the examples/simformer folder of the synference repo.\n",
    "\n",
    "There are some limitations to the current sbi simformer implementation. Currently it doesn't seem to support serialization (due to the use of lambda functions), so models cannot be saved and loaded like other synference SBI models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6284d69d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from synference import SBI_Fitter\n",
    "\n",
    "fitter = SBI_Fitter.init_from_hdf5(model_name=\"test\",\n",
    "                                   hdf5_path=\"../example_grids/test_model_grid.hdf5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88192510",
   "metadata": {},
   "source": [
    "We will create our training array as normal using the ```SBI_Fitter``` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96c2ff30",
   "metadata": {},
   "outputs": [],
   "source": [
    "fitter.create_feature_array();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe118ab9",
   "metadata": {},
   "source": [
    "There are a few parameters to be aware of:\n",
    "\n",
    "1. ```sde_type``` : The type of SDE to use. Options are 've` (variance exploding), 'vp' (variance preserving) or 'subvp' (sub variance preserving). This doesn't do anything for the flow based simformer.\n",
    "2. ```simformer_type```: 'score' or 'flow'- whether to use a score based or flow based simformer. \n",
    "3  ```learning_rate```: The learning rate to use for training.\n",
    "4. ```model_kwargs```: A dictionary of additional keyword arguments to pass to the simformer model. These can include:\n",
    "    - ```num_layers```: The number of transformer layers to use.\n",
    "    - ```num_heads```: The number of attention heads to use.\n",
    "    - ```dim_val```: The dimension of the value vectors in the attention mechanism.\n",
    "    - ```dim_id```: The dimension of the identity vectors in the attention mechanism.\n",
    "    - ```mlp_ratio``` : The ratio of the hidden dimension to the input dimension in the MLP layers.\n",
    "    - ```hidden_features```: The number of hidden features to use in the MLP layers.\n",
    "    - ```time_embedding_dim```: The dimension of the time embedding.\n",
    "\n",
    "Like all other synference models we can also set the ```training_batch_size```, ```validation_fraction```, ```stop_after_epochs``` and ```clip_max_norm``` parameters.\n",
    "\n",
    "```python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "068c4c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "fitter.run_single_simformer(\n",
    "    name_append=\"simformer_test\",\n",
    "    sde_type=\"ve\",\n",
    "    simformer_type=\"score\",\n",
    "    learning_rate=1e-5,\n",
    "    training_batch_size=64,\n",
    "    model_kwargs={\n",
    "        \"hidden_features\": 128,\n",
    "        \"n_layers\": 6,\n",
    "        \"dim_val\": 64,\n",
    "        \"dim_id\": 64,\n",
    "        \"mlp_ratio\": 4,\n",
    "        \"time_embedding_dim\": 32,\n",
    "        \"num_heads\": 4,\n",
    "    },\n",
    "    load_existing_model=False,\n",
    "    validation_fraction=0.1,\n",
    "    stop_after_epochs=30,\n",
    "    plot=False,  # Currently the LtU-ILI plotting doesn't work with the simformer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e527bead",
   "metadata": {},
   "outputs": [],
   "source": [
    "?fitter.posteriors"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
