{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2291bddd",
   "metadata": {},
   "source": [
    "### The Simformer\n",
    "\n",
    "[Gloeckler et al. 2024](https://arxiv.org/abs/2404.09636) introduced the Simformer, for 'all in one simulation based inference'.\n",
    "\n",
    "They use a novel probablistic diffusion model with a transformer architecture which learns the full joint distribution of parameters and data, allowing for fast, amortized Bayesian inference, without specifying beforehand which parameters are of interest. This makes the simformer approach particularly well suited to missing data, as the use of an attention mechanism allows the model sample from an arbitrary conditional distribution excluding any missing data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12ddb0a9",
   "metadata": {},
   "source": [
    "The Simformer is currently implemented in two ways. The first way, is a seperate class called ```Simformer_Fitter```, which requires the user to install a [fork of the original simformer repo](https://github.com/tHarvey303/simformer/), which requires quite specific versions of CUDA, PyTorch and jax to work. \n",
    "\n",
    "The second way uses the new simformer implementation in the sbi package, which is currently only available in a pull request, but should be merged into the main branch soon. For now we will deal with this implementation, as it is much easier to install and use. There are examples of using the original approach in the examples/simformer folder of the synference repo.\n",
    "\n",
    "There are some limitations to the current sbi simformer implementation. Currently it doesn't seem to support serialization (due to the use of lambda functions), so models cannot be saved and loaded like other synference SBI models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6284d69d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/cosma/apps/dp276/dc-harv3/venv_simformer/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from synference import SBI_Fitter\n",
    "\n",
    "fitter = SBI_Fitter.init_from_hdf5(model_name=\"test\",\n",
    "                                   hdf5_path=\"../example_grids/test_model_grid.hdf5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88192510",
   "metadata": {},
   "source": [
    "We will create our training array as normal using the ```SBI_Fitter``` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "96c2ff30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-11-06 00:03:49,586 | synference | INFO     | ---------------------------------------------\n",
      "2025-11-06 00:03:49,588 | synference | INFO     | Features: 8 features over 100 samples\n",
      "2025-11-06 00:03:49,588 | synference | INFO     | ---------------------------------------------\n",
      "2025-11-06 00:03:49,589 | synference | INFO     | Feature: Min - Max\n",
      "2025-11-06 00:03:49,589 | synference | INFO     | ---------------------------------------------\n",
      "2025-11-06 00:03:49,590 | synference | INFO     | JWST/NIRCam.F070W: 7.131974 - 42.758 AB\n",
      "2025-11-06 00:03:49,591 | synference | INFO     | JWST/NIRCam.F090W: 7.108530 - 39.933 AB\n",
      "2025-11-06 00:03:49,591 | synference | INFO     | JWST/NIRCam.F115W: 7.012560 - 38.354 AB\n",
      "2025-11-06 00:03:49,592 | synference | INFO     | JWST/NIRCam.F150W: 6.969396 - 36.997 AB\n",
      "2025-11-06 00:03:49,593 | synference | INFO     | JWST/NIRCam.F200W: 7.133157 - 35.470 AB\n",
      "2025-11-06 00:03:49,593 | synference | INFO     | JWST/NIRCam.F277W: 7.670149 - 33.243 AB\n",
      "2025-11-06 00:03:49,593 | synference | INFO     | JWST/NIRCam.F356W: 8.072730 - 32.490 AB\n",
      "2025-11-06 00:03:49,594 | synference | INFO     | JWST/NIRCam.F444W: 8.353975 - 31.965 AB\n",
      "2025-11-06 00:03:49,594 | synference | INFO     | ---------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "fitter.create_feature_array();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe118ab9",
   "metadata": {},
   "source": [
    "There are a few parameters to be aware of:\n",
    "\n",
    "1. ```sde_type``` : The type of SDE to use. Options are 've` (variance exploding), 'vp' (variance preserving) or 'subvp' (sub variance preserving). This doesn't do anything for the flow based simformer.\n",
    "2. ```simformer_type```: 'score' or 'flow'- whether to use a score based or flow based simformer. \n",
    "3  ```learning_rate```: The learning rate to use for training.\n",
    "4. ```model_kwargs```: A dictionary of additional keyword arguments to pass to the simformer model. These can include:\n",
    "    - ```num_layers```: The number of transformer layers to use.\n",
    "    - ```num_heads```: The number of attention heads to use.\n",
    "    - ```dim_val```: The dimension of the value vectors in the attention mechanism.\n",
    "    - ```dim_id```: The dimension of the identity vectors in the attention mechanism.\n",
    "    - ```mlp_ratio``` : The ratio of the hidden dimension to the input dimension in the MLP layers.\n",
    "    - ```hidden_features```: The number of hidden features to use in the MLP layers.\n",
    "    - ```time_embedding_dim```: The dimension of the time embedding.\n",
    "\n",
    "Like all other synference models we can also set the ```training_batch_size```, ```validation_fraction```, ```stop_after_epochs``` and ```clip_max_norm``` parameters.\n",
    "\n",
    "```python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "068c4c59",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'FlowMatchingSimformer' from 'sbi.inference' (/cosma/apps/dp276/dc-harv3/venv_simformer/lib/python3.12/site-packages/sbi/inference/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mfitter\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_single_simformer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[43m    \u001b[49m\u001b[43mname_append\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msimformer_test\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m    \u001b[49m\u001b[43msde_type\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mve\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43msimformer_type\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mscore\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1e-5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtraining_batch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m64\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mhidden_features\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m128\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mn_layers\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m6\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdim_val\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m64\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdim_id\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m64\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmlp_ratio\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m4\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtime_embedding_dim\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m32\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mnum_heads\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m4\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[43m    \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[43m    \u001b[49m\u001b[43mload_existing_model\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvalidation_fraction\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstop_after_epochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m30\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     19\u001b[39m \u001b[43m    \u001b[49m\u001b[43mplot\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Currently the LtU-ILI plotting doesn't work with the simformer\u001b[39;49;00m\n\u001b[32m     20\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/cosma/apps/dp276/dc-harv3/synference/src/synference/sbi_runner.py:3998\u001b[39m, in \u001b[36mSBI_Fitter.run_single_simformer\u001b[39m\u001b[34m(self, train_test_fraction, random_seed, train_indices, test_indices, save_model, verbose, out_dir, plot, name_append, save_method, set_self, override_prior_ranges, load_existing_model, use_existing_indices, evaluate_model, max_num_epochs, sde_type, simformer_type, model_kwargs, learning_rate, training_batch_size, validation_fraction, stop_after_epochs, clip_max_norm, training_args)\u001b[39m\n\u001b[32m   3926\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mrun_single_simformer\u001b[39m(\n\u001b[32m   3927\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   3928\u001b[39m     train_test_fraction: \u001b[38;5;28mfloat\u001b[39m = \u001b[32m0.8\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   3952\u001b[39m     training_args: \u001b[38;5;28mdict\u001b[39m = {},\n\u001b[32m   3953\u001b[39m ) -> \u001b[38;5;28mtuple\u001b[39m:\n\u001b[32m   3954\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"Trains a single Simformer model using the SBI implementation.\u001b[39;00m\n\u001b[32m   3955\u001b[39m \n\u001b[32m   3956\u001b[39m \u001b[33;03m    May need to be on this branch:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   3996\u001b[39m \u001b[33;03m        training_args: Additional arguments to pass to the training function.\u001b[39;00m\n\u001b[32m   3997\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m3998\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msbi\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01minference\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m FlowMatchingSimformer, Simformer\n\u001b[32m   4000\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.has_features, (\n\u001b[32m   4001\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mFeature array not created. Please create the feature array first.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   4002\u001b[39m     )\n\u001b[32m   4004\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.fitted_parameter_array \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[31mImportError\u001b[39m: cannot import name 'FlowMatchingSimformer' from 'sbi.inference' (/cosma/apps/dp276/dc-harv3/venv_simformer/lib/python3.12/site-packages/sbi/inference/__init__.py)"
     ]
    }
   ],
   "source": [
    "fitter.run_single_simformer(\n",
    "    name_append=\"simformer_test\",\n",
    "    sde_type=\"ve\",\n",
    "    simformer_type=\"score\",\n",
    "    learning_rate=1e-5,\n",
    "    training_batch_size=64,\n",
    "    model_kwargs={\n",
    "        \"hidden_features\": 128,\n",
    "        \"n_layers\": 6,\n",
    "        \"dim_val\": 64,\n",
    "        \"dim_id\": 64,\n",
    "        \"mlp_ratio\": 4,\n",
    "        \"time_embedding_dim\": 32,\n",
    "        \"num_heads\": 4,\n",
    "    },\n",
    "    load_existing_model=False,\n",
    "    validation_fraction=0.1,\n",
    "    stop_after_epochs=30,\n",
    "    plot=False,  # Currently the LtU-ILI plotting doesn't work with the simformer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e527bead",
   "metadata": {},
   "outputs": [],
   "source": [
    "?fitter.posteriors"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_simformer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
