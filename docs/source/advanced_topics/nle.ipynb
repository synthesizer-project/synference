{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f34939c2",
   "metadata": {},
   "source": [
    "# NLE and Nested Sampling\n",
    "\n",
    "Neural Likelihood Estimation (NLE) is a simulation-based inference method that focuses on estimating the likelihood function directly using neural networks. This approach is particularly useful when the likelihood is complex or intractable, allowing for efficient parameter inference.\n",
    "\n",
    "NLE must be combined with a sampling method to draw samples from the posterior distribution. Nested Sampling is one such method that is well-suited for this purpose, as it efficiently explores the parameter space and estimates the evidence. Evidence estimation is crucial for model comparison and selection in Bayesian inference.\n",
    "\n",
    "Note that the following code currently requires a custom fork of the blackjax library which include nested sampling functionality. This fork can be installed via pip:\n",
    "\n",
    "```bash\n",
    "pip install git+https://github.com/handley-lab/blackjax\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94ee377a",
   "metadata": {},
   "source": [
    "The following implementation of NLE combined with Nested Sampling is taken from here: https://handley-lab.co.uk/nested-sampling-book/sbi/nle_posterior.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "361d5543",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "\n",
    "import anesthetic\n",
    "import blackjax\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "import torch\n",
    "import tqdm\n",
    "from blackjax.ns.utils import finalise\n",
    "from sbi.inference.posteriors.base_posterior import NeuralPosterior\n",
    "from sbi.inference.potentials.likelihood_based_potential import LikelihoodBasedPotential\n",
    "\n",
    "\n",
    "class NSPosterior(NeuralPosterior):\n",
    "    \"\"\"Nested Sampling posterior for sbi.\n",
    "\n",
    "    Uses BlackJAX nested sampling to sample from posterior distributions\n",
    "    when given a likelihood estimator and prior.\n",
    "\n",
    "    Args:\n",
    "        likelihood: Trained likelihood estimator from NLE\n",
    "        prior: Prior distribution\n",
    "        num_live: Number of live points for nested sampling\n",
    "        num_inner_steps: Number of slice sampling steps\n",
    "        x_o: Observed data (can be set later with set_default_x)\n",
    "        num_delete: Number of points to delete per iteration (default: 1)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        likelihood,\n",
    "        prior,\n",
    "        num_live,\n",
    "        num_inner_steps,\n",
    "        x_o=None,\n",
    "        num_delete=1,\n",
    "        columns=None,\n",
    "    ):\n",
    "        \"\"\"Initialize NSPosterior instance.\n",
    "\n",
    "        Args:\n",
    "            likelihood: Trained likelihood estimator from NLE\n",
    "            prior: Prior distribution\n",
    "            num_live: Number of live points for nested sampling\n",
    "            num_inner_steps: Number of slice sampling steps\n",
    "            x_o: Observed data (can be set later with set_default_x)\n",
    "            num_delete: Number of points to delete per iteration (default: 1)\n",
    "            columns: List of column names for anesthetic NestedSamples object\n",
    "        \"\"\"\n",
    "        self.num_live = num_live\n",
    "        self.num_delete = num_delete\n",
    "        self.num_inner_steps = num_inner_steps\n",
    "        self.columns = columns\n",
    "\n",
    "        potential_fn = LikelihoodBasedPotential(likelihood, prior, x_o)\n",
    "        super().__init__(potential_fn)\n",
    "\n",
    "    def _loglikelihood_fn(self, theta):\n",
    "        \"\"\"Compute log-likelihood for BlackJAX nested sampling.\"\"\"\n",
    "        x_o_batch = self.default_x.unsqueeze(0).expand(1, theta.shape[0], -1)\n",
    "        return self.potential_fn.likelihood_estimator.log_prob(x_o_batch, condition=theta).squeeze(\n",
    "            0\n",
    "        )\n",
    "\n",
    "    def _logprior_fn(self, theta):\n",
    "        \"\"\"Compute log-prior for BlackJAX nested sampling.\"\"\"\n",
    "        return self.potential_fn.prior.log_prob(theta)\n",
    "\n",
    "    def nested_samples(self) -> anesthetic.NestedSamples:\n",
    "        \"\"\"Run BlackJAX nested sampling and return anesthetic NestedSamples object.\n",
    "\n",
    "        Returns:\n",
    "            NestedSamples object containing particles, log-likelihoods, and birth\n",
    "            log-likelihoods that can be used for evidence calculation and diagnostics.\n",
    "        \"\"\"\n",
    "\n",
    "        def wrap_fn(fn, vmap_method=\"legacy_vectorized\"):\n",
    "            \"\"\"Wrap a PyTorch function to be JAX-compatible using jax.pure_callback.\"\"\"\n",
    "\n",
    "            def numpy_wrapper(theta):\n",
    "                \"\"\"Convert inputs and outputs between JAX and PyTorch/numpy.\"\"\"\n",
    "                x = torch.from_numpy(np.asarray(theta).copy()).float()\n",
    "                result = fn(x)\n",
    "                return result.detach().numpy()\n",
    "\n",
    "            def jax_wrapper(x):\n",
    "                \"\"\"JAX wrapper that uses pure_callback to call numpy_wrapper.\"\"\"\n",
    "                out_shape = jax.ShapeDtypeStruct(x.shape[:-1], x.dtype)\n",
    "                return jax.pure_callback(numpy_wrapper, out_shape, x, vmap_method=vmap_method)\n",
    "\n",
    "            return jax_wrapper\n",
    "\n",
    "        algo = blackjax.nss(\n",
    "            logprior_fn=wrap_fn(self._logprior_fn),\n",
    "            loglikelihood_fn=wrap_fn(self._loglikelihood_fn),\n",
    "            num_delete=self.num_delete,\n",
    "            num_inner_steps=self.num_inner_steps,\n",
    "        )\n",
    "        prior_samples = self.potential_fn.prior.sample((self.num_live,))\n",
    "        initial_live = jnp.array(prior_samples)\n",
    "\n",
    "        rng_key = jax.random.PRNGKey(42)\n",
    "        live = algo.init(initial_live)\n",
    "        step = jax.jit(algo.step)\n",
    "\n",
    "        dead_points = []\n",
    "\n",
    "        with tqdm.tqdm(desc=\"Dead points\", unit=\" dead points\") as pbar:\n",
    "            while not live.logZ_live - live.logZ < -3:\n",
    "                rng_key, subkey = jax.random.split(rng_key)\n",
    "                live, dead = step(subkey, live)\n",
    "                dead_points.append(dead)\n",
    "                pbar.update(len(dead.particles))\n",
    "\n",
    "        ns_run = finalise(live, dead_points)\n",
    "\n",
    "        return anesthetic.NestedSamples(\n",
    "            data=ns_run.particles,\n",
    "            logL=ns_run.loglikelihood,\n",
    "            logL_birth=ns_run.loglikelihood_birth,\n",
    "            columns=self.columns,\n",
    "        )\n",
    "\n",
    "    def sample(\n",
    "        self, sample_shape: torch.Size = torch.Size(), x: Optional[torch.Tensor] = None, **kwargs\n",
    "    ) -> torch.Tensor:\n",
    "        r\"\"\"Return samples from posterior distribution $p(\\theta|x)$ with nested sampling.\n",
    "\n",
    "        Check the `__init__()` method for a description of all arguments as well as\n",
    "        their default values.\n",
    "\n",
    "        Args:\n",
    "            sample_shape: Desired shape of samples that are drawn from posterior. If\n",
    "                sample_shape is multidimensional we simply draw `sample_shape.numel()`\n",
    "                samples and then reshape into the desired shape.\n",
    "            x: Observation to condition on. If None, uses default_x set via\n",
    "                `set_default_x()`.\n",
    "            kwargs: Additional keyword arguments (not used).\n",
    "\n",
    "        Returns:\n",
    "            Samples from posterior with shape `(*sample_shape, theta_dim)`.\n",
    "        \"\"\"\n",
    "        self.potential_fn.set_x(self._x_else_default_x(x))\n",
    "        ns = self.nested_samples()\n",
    "        samples = ns.sample(torch.Size(sample_shape).numel())\n",
    "        samples_array = samples.drop(columns=[\"logL\", \"logL_birth\", \"nlive\"]).values\n",
    "        return torch.from_numpy(samples_array).reshape((*sample_shape, -1))\n",
    "\n",
    "    def sample_batched(self, sample_shape, x, **kwargs):\n",
    "        \"\"\"Placeholder for batched sampling (not implemented).\"\"\"\n",
    "        raise NotImplementedError(\"Batched sampling not available for nested sampling.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f323fdf5",
   "metadata": {},
   "source": [
    "A brief summary of the key parameters used in the nested sampling implementation:\n",
    "- n_live – the number of live points, typically set to some multiple of the dimension of the parameter space, 25x D is a good starting point\n",
    "- num_inner_steps – the length of the short Markov chains used to update the population, typically set to some multiple of the dimension of the parameter space, 3x D is a good starting point\n",
    "- num_delete – some integer [1 - n_live], how many chains to vectorize over, setting this to approximately 10% of n_live is likely good for CPU usage, Up to 50% for GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "970f1fe7",
   "metadata": {},
   "source": [
    "We can now load in a previously trained NLE model. In Synference, an NLE model can be trained in the same way as an NLE model, by using the `SBI_Fitter` class with the `engine='NLE'` argument. Here, we load in a previously trained NLE model from disk."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff9d2649",
   "metadata": {},
   "source": [
    "We can access the likelihood estimator from the trained model using the `likelihood_func` property of the `SBI_Fitter` class.\n",
    "\n",
    "We will demonstate the utility of NLE for model comparison by comparing two different stellar population synthesis models: BPASS and FSPS. We will load in previously trained NLE models for both BPASS and FSPS, and use Nested Sampling to draw samples from the posterior distribution for a given observed data point. We will then compute the evidence for each model using the samples drawn from the posterior.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e7d6fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from anesthetic import NestedSamples\n",
    "from pandas import read_csv\n",
    "\n",
    "from synference import SBI_Fitter\n",
    "\n",
    "bpass_model = SBI_Fitter.load_saved_model(\"BPASS_NLE_v4\", device=\"cpu\")\n",
    "\n",
    "prior_bpass = bpass_model.create_priors()\n",
    "\n",
    "posterior_bpass = NSPosterior(\n",
    "    bpass_model.likelihood_func,\n",
    "    prior_bpass,\n",
    "    num_live=1000,\n",
    "    num_inner_steps=30,\n",
    "    num_delete=100,\n",
    "    columns=bpass_model.fitted_parameter_names,\n",
    ")\n",
    "\n",
    "fsps_model = SBI_Fitter.load_saved_model(\"FSPS_NLE_v4\", device=\"cpu\")\n",
    "prior_fsps = fsps_model.create_priors()\n",
    "\n",
    "posterior_fsps = NSPosterior(\n",
    "    fsps_model.likelihood_func,\n",
    "    prior_fsps,\n",
    "    num_live=1000,\n",
    "    num_inner_steps=30,\n",
    "    num_delete=100,\n",
    "    columns=fsps_model.fitted_parameter_names,\n",
    ")\n",
    "\n",
    "x_o_fsps = fsps_model._X_test[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0edb0cbe",
   "metadata": {},
   "source": [
    "Here is our chosen observed data point from the FSPS test set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b2bd7b5",
   "metadata": {},
   "source": [
    "We set our observed data for sampling using the `.set_default_x()` method of the `NSPosterior` class. We can then draw samples and compute evidences from the posterior using the `.nested_samples()` method.\n",
    "\n",
    "We can then compute Bayes factor, the ratio of evidences, to compare the two models. A Bayes factor greater than 1 indicates that the data favors the BPASS model over the FSPS model, while a Bayes factor less than 1 indicates the opposite."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8732eeab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "from pandas import read_csv\n",
    "\n",
    "if not os.path.exists(\"bpass_nested_samples.csv\"):\n",
    "    posterior_bpass.set_default_x(x_o_fsps)\n",
    "    bpass_samples = posterior_bpass.nested_samples()\n",
    "    bpass_samples.to_csv(\"bpass_nested_samples.csv\")\n",
    "else:\n",
    "    bpass_samples = NestedSamples(read_csv(\"bpass_nested_samples.csv\"))\n",
    "\n",
    "\n",
    "print(f\"Evidence for BPASS: {bpass_samples.logZ().mean():.2f} ± {bpass_samples.logZ().std():.2f}\")\n",
    "\n",
    "if not os.path.exists(\"fsps_nested_samples.csv\"):\n",
    "    posterior_fsps.set_default_x(x_o_fsps)\n",
    "    fsps_samples = posterior_fsps.nested_samples()\n",
    "    fsps_samples.to_csv(\"fsps_nested_samples.csv\")\n",
    "else:\n",
    "    fsps_samples = NestedSamples(read_csv(\"fsps_nested_samples.csv\"))\n",
    "\n",
    "\n",
    "print(f\"Evidence for FSPS: {fsps_samples.logZ().mean():.2f} ± {fsps_samples.logZ().std():.2f}\")\n",
    "\n",
    "bayes_factor = np.exp(fsps_samples.logZ().mean() - bpass_samples.logZ().mean())\n",
    "print(f\"Bayes factor (FSPS / BPASS): {bayes_factor:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53766d75",
   "metadata": {},
   "source": [
    "We can plot the corner plot of the samples using the `anesthetic` package as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "717f248c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from anesthetic import make_2d_axes\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "plt.rcParams.update({\"figure.dpi\": 150})\n",
    "\n",
    "params = [\n",
    "    \"log_mass\",\n",
    "    \"log10metallicity\",\n",
    "    \"log10_Av\",\n",
    "    \"log_sfr\",\n",
    "    \"sfh_quantile_25\",\n",
    "    \"sfh_quantile_50\",\n",
    "    \"sfh_quantile_75\",\n",
    "]\n",
    "fig, axes = make_2d_axes(params, upper=False, figsize=(10, 10))\n",
    "\n",
    "truth = fsps_model._y_test[1]\n",
    "axes.scatter(\n",
    "    {param: truth[i] for i, param in enumerate(params)},\n",
    "    c=\"black\",\n",
    "    marker=\"x\",\n",
    "    s=100,\n",
    "    label=\"Truth (FSPS)\",\n",
    ")\n",
    "bpass_samples.plot_2d(axes, c=\"red\", fc=None, label=\"BPASS NLE+NS posterior\")\n",
    "fsps_samples.plot_2d(axes, c=\"blue\", fc=None, label=\"FSPS NLE+NS posterior\")\n",
    "\n",
    "axes.iloc[-1, 0].legend(loc=\"upper right\", bbox_to_anchor=(len(axes), len(axes)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e3532ac",
   "metadata": {},
   "source": [
    "We can compare this to a trained NPE model as well, which will directly provide posterior samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0c14597",
   "metadata": {},
   "outputs": [],
   "source": [
    "from synference import load_unc_model_from_hdf5\n",
    "\n",
    "grid_path = \"../example_models/bpass_db_v4/grid_BPASS_Chab_DenseBasis_SFH_0.01_z_14_logN_2.7_Calzetti_v3_multinode.hdf5\"  # noqa: E501\n",
    "\n",
    "fitter = SBI_Fitter.load_saved_model(\n",
    "    model_file=\"../example_models/bpass_db_v4\", grid_path=grid_path, device=\"cpu\"\n",
    ")\n",
    "\n",
    "nm_path = (\n",
    "    \"../example_models/bpass_db_v4/BPASS_DenseBasis_v4_final_nsf_0_params_empirical_noise_models.h5\"\n",
    ")\n",
    "noise_models = load_unc_model_from_hdf5(nm_path)\n",
    "\n",
    "fitter.feature_array_flags[\"empirical_noise_models\"] = noise_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14998e2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = fitter.sample_posterior(x_o_fsps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec8a3f49",
   "metadata": {},
   "source": [
    "Now we can load our NPE samples into anesthetic and plot the corner plot for comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57c2269b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from anesthetic.samples import Samples\n",
    "\n",
    "num_samples = samples.shape[0]\n",
    "params = fsps_model.fitted_parameter_names[:7]\n",
    "samples = samples[:, :7]\n",
    "num_dim = samples.shape[1]\n",
    "\n",
    "npe_samples = Samples(\n",
    "    data=samples,\n",
    "    columns=params,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99257945",
   "metadata": {},
   "source": [
    "Here is the corner plot comparison of NPE (using a BPASS model) and NLE + Nested Sampling (using a FSPS model and a BPASS model). The test data point was generated from the FSPS model, so we expect the NLE + Nested Sampling with FSPS to perform better than the NPE with BPASS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "807e52d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "npe_samples.plot_2d(axes, c=\"green\", fc=None, label=\"BPASS NPE posterior\")\n",
    "axes.iloc[-1, 0].legend(loc=\"upper right\", bbox_to_anchor=(len(axes), len(axes)))\n",
    "\n",
    "fig"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
