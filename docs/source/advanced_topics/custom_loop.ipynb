{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "72b87783",
   "metadata": {},
   "source": [
    "# Custom Training Loop\n",
    "\n",
    "Synference implements a custom training loop (within the `custom_runner.py` submodule) which offers more flexibility than the built-in training loops of `sbi` and `LtU-ILI`. This allows users to implement advanced training strategies, such as custom optimizers (e.g. AdamW), and several quality of life features, such as model caching during training to avoid losing progress in case of interruptions.\n",
    "\n",
    "Crucially, this custom training loop is also directly integrated with Optuna for hyperparameter optimization, allowing users to easily perform hyperparameter searches while training their models. It reports training progress to Optuna, allowing users to monitor the performance of different hyperparameter configurations in real-time, and allows pruning of unpromising trials based on intermediate results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2843c835",
   "metadata": {},
   "source": [
    "The interface to this is still in beta, and will be stabilized in future releases and brought into line with the rest of Synference's API.\n",
    "\n",
    "Currently the custom training loop is configured via a YAML configuration file, which specifies the training parameters, optimizer settings, and other options. An example configuration file is provided below:\n",
    "\n",
    "```yaml\n",
    "\n",
    "train_args:\n",
    "  skip_optimization: True\n",
    "  validation_fraction: 0.1\n",
    "  fixed_params:\n",
    "    model_choice: \"nsf\" # Must be a list\n",
    "    optimizer_choice: \"Adam\" # Must be a list\n",
    "    learning_rate: 0.0007460108070908076\n",
    "    training_batch_size: 79\n",
    "    stop_after_epochs: 57\n",
    "    clip_max_norm: 6.656577606872957\n",
    "    nsf_hidden_features: 30\n",
    "    nsf_num_transforms: 14\n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "This configuration file does not perform optimization, but instead trains a model with specified hyperparameters. To perform hyperparameter optimization, an example configuration file is provided below:\n",
    "\n",
    "```yaml\n",
    "\n",
    "train_args:\n",
    "  skip_optimization: False\n",
    "  validation_fraction: 0.1\n",
    "  optuna:\n",
    "    n_trials: 50\n",
    "    build_final_model: False\n",
    "    objective:\n",
    "      metric: 'log_prob'\n",
    "    study:\n",
    "      study_name: \"\"\n",
    "      storage: \"\"\n",
    "      direction: 'maximize'\n",
    "      load_if_exists: True\n",
    "    pruner:\n",
    "      type: Median\n",
    "      n_startup_trials: 10\n",
    "      n_warmup_steps: 30\n",
    "      interval_steps: 10\n",
    "      n_min_trials: 10\n",
    "      #max_resource: 1000\n",
    "      #reduction_factor: 3\n",
    "      #min_resource: 10\n",
    "      #bootstrap_count: 10\n",
    "    search_space:\n",
    "      model_choice: [\"mdn\", \"nsf\"] # Must be a list\n",
    "      optimizer_choice: [\"AdamW\", \"Adam\"] # Must be a list\n",
    "      learning_rate:\n",
    "        type: \"float\"\n",
    "        low: 1e-6\n",
    "        high: 5e-2\n",
    "        log: True\n",
    "      training_batch_size:\n",
    "        type: \"int\"\n",
    "        low: 32\n",
    "        high: 256\n",
    "      stop_after_epochs:\n",
    "        type: \"int\"\n",
    "        low: 10\n",
    "        high: 60\n",
    "      clip_max_norm:\n",
    "        type: \"float\"\n",
    "        low: 0.1\n",
    "        high: 10.0\n",
    "      models:\n",
    "        nsf: \n",
    "          hidden_features:\n",
    "            type: \"int\"\n",
    "            low: 10\n",
    "            high: 100\n",
    "          num_transforms:\n",
    "            type: \"int\"\n",
    "            low: 3\n",
    "            high: 128\n",
    "        mdn: \n",
    "          hidden_features:\n",
    "            type: \"int\"\n",
    "            low: 10\n",
    "            high: 200\n",
    "          num_components:\n",
    "            type: \"int\"\n",
    "            low: 10\n",
    "            high: 600\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "This configuration file will perform hyperparameter optimization over the specified search space, using the median pruner to prune unpromising trials. The objective metric is set to 'log_prob', meaning that the optimization will aim to maximize the log probability of the validation data under the trained model.\n",
    "\n",
    "If you are performing optimization parallelized across multiple nodes, you will probably  want to run an external SQL database which supports concurrent connections, such as PostgreSQL or MySQL, and provide the appropriate connection string in the `storage` field of the `study` section of the configuration file, or directly via the `sql_db_path` argument of the `run_single_sbi()` method."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3145980e",
   "metadata": {},
   "source": [
    "```python\n",
    "\n",
    "fitter.run_single_sbi(...,\n",
    "    custom_config_yaml=\"path/to/custom_config.yaml\",\n",
    "    sql_db_path='mysql+pymysql://root:password@url:port/study_name'\n",
    ")\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6261165f",
   "metadata": {},
   "source": [
    "We recommend the Optuna dashboard for monitoring the progress of hyperparameter optimization. This can be launched using the following command:\n",
    "\n",
    "```bash\n",
    "optuna-dashboard your_sql_link\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
