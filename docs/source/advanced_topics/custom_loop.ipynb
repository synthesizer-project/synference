{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "72b87783",
   "metadata": {},
   "source": [
    "# Custom Training Loop\n",
    "\n",
    "Synference implements a custom training loop (within the `custom_runner.py` submodule) which offers more flexibility than the built-in training loops of `sbi` and `LtU-ILI`. This allows users to implement advanced training strategies, such as custom optimizers (e.g. AdamW), and several quality of life features, such as model caching during training to avoid losing progress in case of interruptions.\n",
    "\n",
    "Crucially, this custom training loop is also directly integrated with Optuna for hyperparameter optimization, allowing users to easily perform hyperparameter searches while training their models. It reports training progress to Optuna, allowing users to monitor the performance of different hyperparameter configurations in real-time, and allows pruning of unpromising trials based on intermediate results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2843c835",
   "metadata": {},
   "source": [
    "The interface to this is still in beta, and will be stabilized in future releases and brought into line with the rest of Synference's API.\n",
    "\n",
    "Currently the custom training loop is configured via a YAML configuration file, which specifies the training parameters, optimizer settings, and other options. An example configuration file is provided below:\n",
    "\n",
    "```yaml\n",
    "\n",
    "train_args:\n",
    "  skip_optimization: True\n",
    "  validation_fraction: 0.1\n",
    "  fixed_params:\n",
    "    model_choice: \"nsf\" # Must be a list\n",
    "    optimizer_choice: \"Adam\" # Must be a list\n",
    "    learning_rate: 0.0007460108070908076\n",
    "    training_batch_size: 79\n",
    "    stop_after_epochs: 57\n",
    "    clip_max_norm: 6.656577606872957\n",
    "    nsf_hidden_features: 30\n",
    "    nsf_num_transforms: 14\n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "This configuration file does not perform optimization, but instead trains a model with specified hyperparameters. To perform hyperparameter optimization, an example configuration file is provided below:\n",
    "\n",
    "```yaml\n",
    "\n",
    "train_args:\n",
    "  skip_optimization: False\n",
    "  validation_fraction: 0.1\n",
    "  optuna:\n",
    "    n_trials: 50\n",
    "    build_final_model: False\n",
    "    objective:\n",
    "      metric: 'log_prob'\n",
    "    study:\n",
    "      study_name: \"\"\n",
    "      storage: \"\"\n",
    "      direction: 'maximize'\n",
    "      load_if_exists: True\n",
    "    pruner:\n",
    "      type: Median\n",
    "      n_startup_trials: 10\n",
    "      n_warmup_steps: 30\n",
    "      interval_steps: 10\n",
    "      n_min_trials: 10\n",
    "      #max_resource: 1000\n",
    "      #reduction_factor: 3\n",
    "      #min_resource: 10\n",
    "      #bootstrap_count: 10\n",
    "    search_space:\n",
    "      model_choice: [\"mdn\", \"nsf\"] # Must be a list\n",
    "      optimizer_choice: [\"AdamW\", \"Adam\"] # Must be a list\n",
    "      learning_rate:\n",
    "        type: \"float\"\n",
    "        low: 1e-6\n",
    "        high: 5e-2\n",
    "        log: True\n",
    "      training_batch_size:\n",
    "        type: \"int\"\n",
    "        low: 32\n",
    "        high: 256\n",
    "      stop_after_epochs:\n",
    "        type: \"int\"\n",
    "        low: 10\n",
    "        high: 60\n",
    "      clip_max_norm:\n",
    "        type: \"float\"\n",
    "        low: 0.1\n",
    "        high: 10.0\n",
    "      models:\n",
    "        nsf: \n",
    "          hidden_features:\n",
    "            type: \"int\"\n",
    "            low: 10\n",
    "            high: 100\n",
    "          num_transforms:\n",
    "            type: \"int\"\n",
    "            low: 3\n",
    "            high: 128\n",
    "        mdn: \n",
    "          hidden_features:\n",
    "            type: \"int\"\n",
    "            low: 10\n",
    "            high: 200\n",
    "          num_components:\n",
    "            type: \"int\"\n",
    "            low: 10\n",
    "            high: 600\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "This configuration file will perform hyperparameter optimization over the specified search space, using the median pruner to prune unpromising trials. The objective metric is set to 'log_prob', meaning that the optimization will aim to maximize the log probability of the validation data under the trained model.\n",
    "\n",
    "If you are performing optimization parallelized across multiple nodes, you will probably  want to run an external SQL database which supports concurrent connections, such as PostgreSQL or MySQL, and provide the appropriate connection string in the `storage` field of the `study` section of the configuration file, or directly via the `sql_db_path` argument of the `run_single_sbi()` method."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3145980e",
   "metadata": {},
   "source": [
    "```python\n",
    "\n",
    "fitter.run_single_sbi(...,\n",
    "    custom_config_yaml=\"path/to/custom_config.yaml\",\n",
    "    sql_db_path='mysql+pymysql://root:password@url:port/study_name'\n",
    ")\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6261165f",
   "metadata": {},
   "source": [
    "We recommend [Optuna Dashboard](https://optuna-dashboard.readthedocs.io/en/latest/getting-started.html) for monitoring the progress of hyperparameter optimization. This can be launched using the following command:\n",
    "\n",
    "```bash\n",
    "optuna-dashboard your_sql_link\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc76e813",
   "metadata": {},
   "source": [
    "## Custom Loop with a Fixed Model\n",
    "\n",
    "You can also use the custom training loop to train models with fixed parameter values, with a different configuration. This lets you take advantage of the additional features such as the customizable optimizer, model training checkpoints and ongoing training monitoring.\n",
    "\n",
    "Here is an example of the yaml file to train a specific fixed model, in this case a Neural Spline Flow, using the Adam optimizer, with 30 hidden features and 14 transforms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb63e847",
   "metadata": {},
   "source": [
    "```yaml\n",
    "train_args:\n",
    "  skip_optimization: True\n",
    "  validation_fraction: 0.1\n",
    "  fixed_params:\n",
    "    model_choice: \"nsf\" # Must be a list\n",
    "    optimizer_choice: \"Adam\" # Must be a list\n",
    "    learning_rate: 0.0007460108070908076\n",
    "    training_batch_size: 79\n",
    "    stop_after_epochs: 57\n",
    "    clip_max_norm: 6.656577606872957\n",
    "    nsf_hidden_features: 30\n",
    "    nsf_num_transforms: 14\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be69d999",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"train_args\": {\n",
    "        \"skip_optimization\": True,\n",
    "        \"validation_fraction\": 0.1,\n",
    "        \"fixed_params\": {\n",
    "            \"model_choice\": \"nsf\",\n",
    "            \"optimizer_choice\": \"Adam\",\n",
    "            \"learning_rate\": 0.0007460108070908076,\n",
    "            \"training_batch_size\": 79,\n",
    "            \"stop_after_epochs\": 57,\n",
    "            \"clip_max_norm\": 6.656577606872957,\n",
    "            \"nsf_hidden_features\": 30,\n",
    "            \"nsf_num_transforms\": 14,\n",
    "        },\n",
    "    }\n",
    "}\n",
    "\n",
    "import yaml\n",
    "\n",
    "with open(\"config.yaml\", \"w\") as f:\n",
    "    yaml.dump(config, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29780005",
   "metadata": {},
   "source": [
    "First we initialize a pre-configured model library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ca21ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from synference import SBI_Fitter, test_data_dir\n",
    "\n",
    "fitter = SBI_Fitter.init_from_hdf5(\n",
    "    model_name=\"test\", hdf5_path=f\"{test_data_dir}/example_model_library.hdf5\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00dae994",
   "metadata": {},
   "source": [
    "Then we create our training arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89b72d4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fitter.create_feature_array();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "429f7303",
   "metadata": {},
   "source": [
    "Now we can train a model - note that this is a terrible model with far too small a dataset, purely for demonstration purposes.\n",
    "\n",
    "The plotted live training loss curves look slightly odd in a notebook format, but you can see the live progress of your model on your training and validation set, and see where the best performing training epoch is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d8c65ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "fitter.run_single_sbi(custom_config_yaml=\"config.yaml\")"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
