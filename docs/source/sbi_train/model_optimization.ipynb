{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Optimization \n",
    "\n",
    "Synference supports [Optuna](https://optuna.org/) for hyperparameter optimization. Optuna is an automatic hyperparameter optimization software framework, particularly designed for machine learning.\n",
    "\n",
    "Optuna works through the creation of a 'Study', which is an optimization task to be executed. Each Study consists of multiple 'Trials', where each Trial represents a single execution of the objective function with a specific set of hyperparameters.\n",
    "\n",
    "Synference supports several different objective functions for optimization, including:\n",
    "- log_probability: Maximizes the log probability of the model given the data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let's create a fitter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from synference import SBI_Fitter\n",
    "\n",
    "fitter = SBI_Fitter.init_from_hdf5(model_name=\"test\", \n",
    "                                   hdf5_path=\"../example_libraries/test_model_library.hdf5\")\n",
    "fitter.create_feature_array(verbose=False);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimizing a model is primarily done through the `optimize_sbi` method of the `SBI_Fitter` class. \n",
    "\n",
    "You must choose a study name, and optionally  a storage location for the study. If no storage location is provided, an in-memory storage will be used, which means that the study will not be saved after the program ends.\n",
    "\n",
    "Optuna works best with a database backend for storage, such as SQLite or PostgreSQL. This allows you to save and resume studies, and also to share studies between different processes or machines. This can be set with the `persistent_storage` parameter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is setup through the configuration of two dictionaries, `suggested_hyperparameters` and `fixed_hyperparameters`. The `suggested_hyperparameters` dictionary contains hyperparameters that will be optimized by Optuna, while the `fixed_hyperparameters` dictionary contains hyperparameters that will remain constant during the optimization process.\n",
    "\n",
    "The keys of these dictionaries correspond to the names of the hyperparameters in the model, and the values in the `suggested_hyperparameters` dictionary are either two component lists/tuples specifying the minimum and maximum values for the hyperparameter, or a list of categorical values to choose from. The values in the `fixed_hyperparameters` dictionary are simply the fixed values for those hyperparameters.\n",
    "\n",
    "We can also set the number of trials to run for the optimization process using the `n_trials` parameter. Note that if you are parallelizing the optimization process across multiple workers, each worker will run `n_trials` trials, so the total number of trials will be `n_trials` multiplied by the number of workers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimization Metrics\n",
    "\n",
    "We must also choose our optimization metric(s). Multi-objective optimization is supported by providing a list of metrics. The available metrics are:\n",
    "- \"log_prob\": Maximizes the log probability of the model given the data.\n",
    "- \"log_prob-pit\": Maximizes the log probability adjusted by the Probability Integral Transform (PIT) to encourage calibrated posteriors.\n",
    "- \"loss\": Minimizes the negative log probability of the model given the data.\n",
    "- Custom callable: You can also provide a custom callable function that takes in the posterior, test data, and test labels, and returns a scalar score.\n",
    "\n",
    "Note that you should set the 'direction' parameter to either 'minimize' or 'maximize' depending on whether you want to minimize or maximize the chosen metric(s)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mSignature:\u001b[0m\n",
      "\u001b[0mfitter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimize_sbi\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mstudy_name\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0msuggested_hyperparameters\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'learning_rate'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1e-06\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.001\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'hidden_features'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m200\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'num_components'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m16\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'training_batch_size'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m128\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'num_transforms'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'stop_after_epochs'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m30\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'clip_max_norm'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5.0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'validation_fraction'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mfixed_hyperparameters\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'n_nets'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'model_type'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'mdn'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mn_trials\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mn_jobs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mrandom_seed\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m42\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mverbose\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mtrain_test_fraction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.9\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mpersistent_storage\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mout_dir\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/Users/user/Documents/PhD/synference/models/'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mscore_metrics\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'log_prob-pit'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mdirection\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'maximize'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mtimeout_minutes_trial_sampling\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfloat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m120.0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0msql_db_path\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDocstring:\u001b[0m\n",
      "Use Optuna to optimize the SBI model hyperparameters.\n",
      "\n",
      "Possible hyperparameters to optimize:\n",
      "- n_nets: Number of networks to use in the ensemble.\n",
      "- model_type: Type of model to use. Either 'mdn' or 'maf'.\n",
      "- hidden_features: Number of hidden features in the neural network.\n",
      "- num_components: Number of components in the mixture density network.\n",
      "- num_transforms: Number of transforms in the masked autoregressive flow.\n",
      "- training_batch_size: Batch size for training.\n",
      "- learning_rate: Learning rate for the optimizer.\n",
      "- validation_fraction: Fraction of the training set to use for validation.\n",
      "- stop_after_epochs: Number of epochs without improvement before stopping.\n",
      "- clip_max_norm: Maximum norm for gradient clipping.\n",
      "\n",
      "Parameters:\n",
      "------------\n",
      "- study_name: Name of the Optuna study.\n",
      "- suggested_hyperparameters: Dictionary of hyperparameters to suggest.\n",
      "    Keys are hyperparameter names and values are lists of possible values.\n",
      "- fixed_hyperparameters: Dictionary of hyperparameters to fix.\n",
      "    Keys are hyperparameter names and values are fixed values.\n",
      "- n_trials: Number of trials to run in the optimization.\n",
      "- n_jobs: Number of parallel jobs to run.\n",
      "    Note that Optuna uses the threading backend, not true parallelism,\n",
      "    so the Python GIL can still be a bottleneck. See\n",
      "    https://optuna.readthedocs.io/en/stable/faq.html#how-can-i-parallelize-optimization\n",
      "    for discussion.\n",
      "- random_seed: Random seed for reproducibility.\n",
      "- verbose: Whether to print progress and results.\n",
      "- persistent_storage: Whether to use persistent storage for the study.\n",
      "- out_dir: Directory to save the study results.\n",
      "- score_metrics: Metrics to use for scoring the trials. Either a string\n",
      "    or a list of metrics.\n",
      "- direction: Direction of optimization, either 'minimize' or 'maximize',\n",
      "    or a list of directions if using multi-objective optimization.\n",
      "- timeout_minutes_trial_sampling: Timeout in minutes for each trial sampling.\n",
      "    e.g. if sampling gets stuck, will prune this trial.\n",
      "- sql_db_path: Optional path to an existing MySQL database for Optuna.\n",
      "    If you don't have one set up, leave as None to create a SQLite database.\n",
      "\u001b[0;31mFile:\u001b[0m      ~/Documents/PhD/synference/src/synference/sbi_runner.py\n",
      "\u001b[0;31mType:\u001b[0m      method"
     ]
    }
   ],
   "source": [
    "?fitter.optimize_sbi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "\n",
    "\n",
    "\n",
    "fitter.optimize_sbi(\n",
    "    study_name=\"my_study\",\n",
    "    suggested_hyperparameters={\n",
    "        \"learning_rate\": (1e-5, 1e-2),  # Optimize learning rate between 1e-5 and 1e-2\n",
    "        \"num_transforms\": [5, 80],     # Optimize number of transforms between 5 and 80\n",
    "        \"hidden_features\": (16, 128),   # Optimize hidden features between 16 and 128\n",
    "    },\n",
    "    fixed_hyperparameters={\n",
    "        \"training_batch_size\": 64,                # Fixed batch size\n",
    "        \"model_type\": \"maf\",                       # Fixed model type\n",
    "    },\n",
    "    n_trials=50,                        # Number of trials to run\n",
    "    score_metrics=\"log_prob\",  # Single-objective optimization\n",
    "    directions='maximize',\n",
    "    persistent_storage=True\n",
    ")\n",
    "\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "astro",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
