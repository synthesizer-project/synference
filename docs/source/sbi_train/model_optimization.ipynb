{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Optimization \n",
    "\n",
    "Synference supports [Optuna](https://optuna.org/) for hyperparameter optimization. Optuna is an automatic hyperparameter optimization software framework, particularly designed for machine learning.\n",
    "\n",
    "Optuna works through the creation of a 'Study', which is an optimization task to be executed. Each Study consists of multiple 'Trials', where each Trial represents a single execution of the objective function with a specific set of hyperparameters.\n",
    "\n",
    "Synference supports several different objective functions for optimization, including:\n",
    "- log_probability: Maximizes the log probability of the model given the data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let's create a fitter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from synference import SBI_Fitter\n",
    "\n",
    "fitter = SBI_Fitter.init_from_hdf5(\n",
    "    model_name=\"test\", hdf5_path=\"../example_libraries/example_model_library.hdf5\"\n",
    ")\n",
    "fitter.create_feature_array(verbose=False);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimizing a model is primarily done through the `optimize_sbi` method of the `SBI_Fitter` class. \n",
    "\n",
    "You must choose a study name, and optionally  a storage location for the study. If no storage location is provided, an in-memory storage will be used, which means that the study will not be saved after the program ends.\n",
    "\n",
    "Optuna works best with a database backend for storage, such as SQLite or PostgreSQL. This allows you to save and resume studies, and also to share studies between different processes or machines. This can be set with the `persistent_storage` parameter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is setup through the configuration of two dictionaries, `suggested_hyperparameters` and `fixed_hyperparameters`. The `suggested_hyperparameters` dictionary contains hyperparameters that will be optimized by Optuna, while the `fixed_hyperparameters` dictionary contains hyperparameters that will remain constant during the optimization process.\n",
    "\n",
    "The keys of these dictionaries correspond to the names of the hyperparameters in the model, and the values in the `suggested_hyperparameters` dictionary are either two component lists/tuples specifying the minimum and maximum values for the hyperparameter, or a list of categorical values to choose from. The values in the `fixed_hyperparameters` dictionary are simply the fixed values for those hyperparameters.\n",
    "\n",
    "We can also set the number of trials to run for the optimization process using the `n_trials` parameter. Note that if you are parallelizing the optimization process across multiple workers, each worker will run `n_trials` trials, so the total number of trials will be `n_trials` multiplied by the number of workers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimization Metrics\n",
    "\n",
    "We must also choose our optimization metric(s). Multi-objective optimization is supported by providing a list of metrics. The available metrics are:\n",
    "- \"log_prob\": Maximizes the log probability of the model given the data.\n",
    "- \"log_prob-pit\": Maximizes the log probability adjusted by the Probability Integral Transform (PIT) to encourage calibrated posteriors.\n",
    "- \"loss\": Minimizes the negative log probability of the model given the data.\n",
    "- Custom callable: You can also provide a custom callable function that takes in the posterior, test data, and test labels, and returns a scalar score.\n",
    "\n",
    "Note that you should set the 'direction' parameter to either 'minimize' or 'maximize' depending on whether you want to minimize or maximize the chosen metric(s)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "?fitter.optimize_sbi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "\n",
    "\n",
    "\n",
    "fitter.optimize_sbi(\n",
    "    study_name=\"my_study\",\n",
    "    suggested_hyperparameters={\n",
    "        \"learning_rate\": (1e-5, 1e-2),  # Optimize learning rate between 1e-5 and 1e-2\n",
    "        \"num_transforms\": [5, 80],     # Optimize number of transforms between 5 and 80\n",
    "        \"hidden_features\": (16, 128),   # Optimize hidden features between 16 and 128\n",
    "    },\n",
    "    fixed_hyperparameters={\n",
    "        \"training_batch_size\": 64,                # Fixed batch size\n",
    "        \"model_type\": \"maf\",                       # Fixed model type\n",
    "    },\n",
    "    n_trials=50,                        # Number of trials to run\n",
    "    score_metrics=\"log_prob\",  # Single-objective optimization\n",
    "    directions='maximize',\n",
    "    persistent_storage=True\n",
    ")\n",
    "\n",
    "```"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}
