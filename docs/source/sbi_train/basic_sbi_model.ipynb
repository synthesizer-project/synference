{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d7b8d481",
   "metadata": {},
   "source": [
    "## Basic SBI Model Training\n",
    "\n",
    "In this tutorial, we will walk through the process of training a simulation-based inference (SBI) model using the `synference` package. We will assume we already have a library of simulations and corresponding parameters.\n",
    "\n",
    "\n",
    "First let's consider the training process more generally. The main steps involved in training an SBI model are:\n",
    "1. **Prepare the Simulation Data**: Gather a set of simulations and their corresponding parameters.\n",
    "2. **Choose a Model Architecture**: Select an appropriate neural network architecture for the SBI model.\n",
    "3. **Define the Training Procedure**: Set up the training loop, loss function, and optimization algorithm.\n",
    "4. **Train the Model**: Run the training process and monitor performance.\n",
    "5. **Evaluate the Model**: Assess the trained model's performance on a validation set.\n",
    "\n",
    "Now let's look at how to implement these steps using `synference`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "260ca46c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from synference import SBI_Fitter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff27c7b1",
   "metadata": {},
   "source": [
    "From the output of the library generation tutorials, we should have a HDF5 file called `test_model_grid.hdf5` in our 'grids/' directory. If you don't have this file, please refer to the [Library Generation](../library_gen/basic_library_generation.ipynb) tutorial.\n",
    "\n",
    "We can directly use this file to instantiate a `SBI_Fitter` instance, which is the class which handles training and evaluating SBI models in `synference`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdbbbbee",
   "metadata": {},
   "outputs": [],
   "source": [
    "fitter = SBI_Fitter.init_from_hdf5(model_name=\"test\", hdf5_path=\"test_model_grid.hdf5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4c8a77d",
   "metadata": {},
   "source": [
    "# Feature and Parameter Arrays"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c3fef41",
   "metadata": {},
   "source": [
    "Now this fitter has loaded the generated observations and parameters from the HDF5 file. Note that the data is not yet normalized or set up with the correct features for training. We will handle that in the next steps.\n",
    "\n",
    "We can see the names of the observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2f7d053",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(fitter.raw_observation_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17b9a933",
   "metadata": {},
   "source": [
    "The names of the features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b5d6975",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(fitter.parameter_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e10a9fe",
   "metadata": {},
   "source": [
    "and any associated parameters units:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "331940e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(fitter.parameter_units)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15067ab7",
   "metadata": {},
   "source": [
    "The actual array itself is stored in the `parameter_array` attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfba6dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(fitter.parameter_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "858e7999",
   "metadata": {},
   "source": [
    "And a similar logic for the observations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "164745d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(fitter.raw_observation_grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7c0f3da",
   "metadata": {},
   "source": [
    "The first step is to turn this raw grid of photometric observations into a set of features that can be used for training. This is done with the `fitter.create_feature_array` method.\n",
    "\n",
    "This method handles the following tasks:\n",
    "1. Normalizing the observations (e.g., converting magnitudes to fluxes, normalizing by a reference band, etc.)\n",
    "2. Creating features from the observations (e.g., colors, ratios, etc.)\n",
    "3. Removing photometric bands in the library from the feature array that are not present in the observations.\n",
    "3. Handling missing data (e.g., setting features to NaN if any of the required bands are missing)\n",
    "4. Adding additional features (e.g., redshift) from the parameter array to the feature array.\n",
    "5. Adding realistic noise to the features based on a provided noise model (see the [Noise Models](../noise_modelling/noise_models.ipynb) tutorial for more details).\n",
    "6. Adding photometric uncertainties to the feature array.\n",
    "\n",
    "\n",
    "The default configuration of this method doesn't do all of these however. By default, all photometric bands are kept, no additional features are added, and no noise is added. The default normalization is to convert the raw array of photometry to AB magnitudes only. \n",
    "\n",
    "We call the method below and we can see it prints information about the features it creates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5da4d2f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "fitter.create_feature_array();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e254d0bc",
   "metadata": {},
   "source": [
    "We will proceed with the default configuration for now. More advanced configurations will be covered in later tutorials. Using different normalizations/units or adding additional features can have a significant impact on the performance of the trained SBI model.\n",
    "\n",
    "Before we do any fitting, we can inspect the feature and parameter arrays to see the distribution of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fcce393",
   "metadata": {},
   "source": [
    "Firstly we can look at the feature array, and see the distribution of the photometry given our model and feature array configuration. The below figure shows a histogram of each feature in the feature array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dc41687",
   "metadata": {},
   "outputs": [],
   "source": [
    "fitter.plot_histogram_feature_array(bins=20);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0104868c",
   "metadata": {},
   "source": [
    "Secondly we can look at the parameter array, and see the distribution of the parameters given our model and parameter configuration. The below figure shows a histogram of each parameter in the parameter array. We can see that the parameters are uniformly distributed, as expected from our library generation configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7be1ca8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fitter.plot_histogram_parameter_array();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "540e8581",
   "metadata": {},
   "source": [
    "# Training an SBI Model\n",
    "\n",
    "SBI model training is handled with the `fitter.train_single_sbi` method. This method handles the following tasks:\n",
    "1. Creating a prior from the parameter array.\n",
    "2. Setting up the neural density estimator (NDE) for the SBI model.\n",
    "3. Training the SBI model.\n",
    "4. Saving the trained model to disk.\n",
    "5. Plotting diagnostics of the trained model.\n",
    "\n",
    "\n",
    "We will cover the various options for different SBI configurations in later tutorials. For now, we will proceed with the default configuration.\n",
    "\n",
    "synference is built on top of the LtU-ILI package, which utilizes `sbi` and `lampe` for the underlying SBI functionality. The default NDE is a `Masked Autoregressive Flow (MAF)` from the `sbi` package. The default prior proposal is a uniform prior over the range of the parameters in the parameter array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20246e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "?fitter.run_single_sbi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3e85812",
   "metadata": {},
   "source": [
    "The primary arguments to the `fitter.train_single_sbi` method are:\n",
    "- `train_test_fraction`: The fraction of the data to use for training. The rest is used for validation. The default is 0.8.\n",
    "- `validation_fraction`: The fraction of the training data to use for validation during training. The default is 0.2.\n",
    "- `backend`: The backend to use for training. Either `sbi` or `lampe`. The default is `sbi`.\n",
    "- `hidden_features`: The number of hidden features in the NDE. The default is 50.\n",
    "- `num_components/transforms`: The number of components or transforms in the NDE. The default is 4.\n",
    "- `training_batch_size`: The batch size for training. The default is 64.\n",
    "- `stop_after_epochs`: The number of epochs with no improvement to stop training. The default is 15.\n",
    "\n",
    "There are other methods to turn on or off plotting, model saving, validation, etc. See the docstring for more details. \n",
    "\n",
    "Now we will run the training, and quite a lot of things will be printed. We are setting `name_append` to 'test_1' so that the trained model is saved with a unique name. If left as the default a timestamp will be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e865904",
   "metadata": {},
   "outputs": [],
   "source": [
    "posterior_model, stats = fitter.run_single_sbi(\n",
    "    name_append=\"test_1\", random_seed=42, hidden_features=256, num_components=64\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0820aca7",
   "metadata": {},
   "source": [
    "\n",
    "The first part of the output shows we split the training data into the training and testing splits, then we create the prior from the parameter grid, and show the ranges of each parameter.\n",
    "\n",
    "The next part shows us creating the neural density estimator (NDE) model, which is a mixture density network (MDN) with 4 components. The model is created using the `sbi` package, which is built on top of PyTorch.\n",
    "\n",
    "Then the actual training happens - we see the training epochs increment until the model has stopped improving on the validation set. The training stops after 15 epochs with no improvement, as we set `stop_after_epochs=15`.\n",
    "\n",
    "The model is pickled and saved to the output directory for this model, which is `models/test_model/` by default. The summary of the training model is saved as a .json file in the same directory. And the configuration of the fitter is also pickled and saved to the same directory, which saves the feature and parameter configuration used for training. A model can be re-loaded later using the `fitter.load_model_from_pkl` method. We can save in a different format by changing the `save_method` argument to e.g. `torch` or `hickle`.\n",
    "\n",
    "Now we have a trained model. The validation metrics run which include:\n",
    "1. A posterior corner plot for a random observation from the test set.\n",
    "2. A loss plot which shows the training and validation loss over epochs.\n",
    "3. A coverage plot which shows how well the credible intervals of the posterior match the true parameters.\n",
    "4. A ranks histogram which shows how well the posterior samples match the true parameters.\n",
    "5. A log_probabiity plot which shows the log probability of the true parameters under the posterior.\n",
    "6. A True vs predicted plot which shows the true parameters vs the maximum a posteriori (MAP) estimate from the posterior.\n",
    "\n",
    "\n",
    "These plots are shown in the output, and also saved to the `plots/` directory in the output folder for this model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cee93fe",
   "metadata": {},
   "source": [
    "# Loading a Trained Model\n",
    "\n",
    "We can load a trained model into an exisiting `SBI_Fitter` instance using the `fitter.load_model_from_pkl` method. This method takes the path to the pickled model file as an argument. \n",
    "\n",
    "If only one model is present in the directory, we can simply provide the directory path and the method will find the model file automatically. If multiple models are present, we can provide the full path to the model file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b523b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fitter.load_model_from_pkl(\"test/test_test_1_posterior.pkl\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2aa95a7",
   "metadata": {},
   "source": [
    "Alternatively, we can create a new `SBI_Fitter` instance and load the model into that instance, using the class method `load_saved_model`. This method takes the path to the pickled model file as an argument, and returns a new `SBI_Fitter` instance with the model loaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cca5b43a",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_fitter = SBI_Fitter.load_saved_model(\"test/test_test_1_posterior.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39e9bbc1",
   "metadata": {},
   "source": [
    "# Plotting model loss\n",
    "\n",
    "We can plot the model loss using the `fitter.plot_loss` method. This method will create a plot of the training and validation loss over epochs, and save it to the `plots/` directory in the output folder for this model. By default, it will not overwrite existing plots, but you can change this with the `overwrite` argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56269c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fitter.plot_loss(overwrite=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5474f94",
   "metadata": {},
   "source": [
    "# Plotting validation metrics\n",
    "\n",
    "Whilst it does happen automatically during training, we can also plot the validation metrics of a trained model using the `fitter.plot_diagnostics` method. You can provide your own validation set, or by default it will use the test set from the last training run. By default, it will not create existing plots in the `plots/` directory, but you can change this with the `overwrite` argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bc27e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fitter.plot_diagnostics();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e42f109",
   "metadata": {},
   "source": [
    "# Getting model metrics\n",
    "\n",
    "We can print and save metrics of the trained model using the `fitter.evaluate_model` method. This method will print the metrics to the console, and also save them to a .json file in the output directory for this model. The metrics include:\n",
    "- TARP (Tests of Accuracy with Random Points)\n",
    "- Log DPIT (Logarithmic Deviation of the Probability Integral Transform)\n",
    "- Mean Log Probability\n",
    "- Parameter-specific metrics (MSE, RMSE, Mean Absolute Error, Median Absolute Error, R-squared, Normalized RMSE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "999bf767",
   "metadata": {},
   "outputs": [],
   "source": [
    "fitter.evaluate_model();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2840471c",
   "metadata": {},
   "source": [
    "# Posterior Samples\n",
    "\n",
    "We can sample the posterior for a given observation using the `fitter.sample_posterior` method. This method takes an observation, or a set of observations, as an argument, and returns samples from the posterior distribution. If no observation is provided, it will draw posterior samples for all observations in the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23093575",
   "metadata": {},
   "outputs": [],
   "source": [
    "fitter.sample_posterior()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61d18ac9",
   "metadata": {},
   "source": [
    "# Next Steps\n",
    "\n",
    "In the next tutorials, we will cover more advanced configurations for training SBI models, including different feature and parameter configurations, different NDEs, and different prior proposals. We will also cover how to use the trained models for inference on real data."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
