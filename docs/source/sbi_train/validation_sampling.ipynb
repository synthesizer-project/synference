{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "55c89da6",
   "metadata": {},
   "source": [
    "# Sampling Validation using MC\n",
    "\n",
    "We can also validate our SBI model by drawing samples from the posterior using MCMC methods. This allows us to assess how well our model captures the underlying distribution of parameters given the observed data.\n",
    "\n",
    "We can do this using the ```SBI_Fitter.fit_observation_using_sampler``` method, which lets us choose from a few nested sampling or MCMC samplers. Here, we'll use the `dynesty` sampler to draw samples from the posterior.\n",
    "\n",
    "First we'll load a trained model and choose a mock observation to fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "289654ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-11-06 17:45:11,190 | synference | INFO     | Loaded model from ../example_models/bpass_db_v4/BPASS_DenseBasis_v4_final_nsf_0_posterior.pkl.\n",
      "2025-11-06 17:45:11,196 | synference | INFO     | Device: cpu\n",
      "2025-11-06 17:45:11,268 | synference | WARNING  | IndexError when trying to set train/test arrays. \n"
     ]
    }
   ],
   "source": [
    "from synference import SBI_Fitter, load_unc_model_from_hdf5\n",
    "\n",
    "library_path = \"../example_models/bpass_db_v4/grid_BPASS_Chab_DenseBasis_SFH_0.01_z_14_logN_2.7_Calzetti_v3_multinode.hdf5\"  # noqa: E501\n",
    "\n",
    "fitter = SBI_Fitter.load_saved_model(\n",
    "    model_file=\"../example_models/bpass_db_v4\", library_path=library_path, device=\"cpu\"\n",
    ")\n",
    "\n",
    "nm_path = (\n",
    "    \"../example_models/bpass_db_v4/BPASS_DenseBasis_v4_final_nsf_0_params_empirical_noise_models.h5\"\n",
    ")\n",
    "noise_models = load_unc_model_from_hdf5(nm_path)\n",
    "\n",
    "fitter.feature_array_flags[\"empirical_noise_models\"] = noise_models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d685bb3",
   "metadata": {},
   "source": [
    "First we can recreate the simulator from the information stored in the grid. We will use the simulator in the log-likelihood calculation during sampling.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0418a828",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overriding internal library name to library passed in directory path.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to load cosmology from HDF5. Using Planck18 instead.\n",
      "params: {'fesc_ly_alpha': 0.0, 'tau_v': 'tau_v', 'dust_curve': <synthesizer.emission_models.transformers.dust_attenuation.Calzetti2000 object at 0x106bb0220>, 'dust_emission': <synthesizer.emission_models.dust.emission.Greybody object at 0x106bb2ce0>}\n",
      "Updated filters: ['HST/ACS_WFC.F435W', 'HST/ACS_WFC.F606W', 'HST/ACS_WFC.F775W', 'HST/ACS_WFC.F814W', 'HST/ACS_WFC.F850LP', 'JWST/NIRCam.F090W', 'JWST/NIRCam.F115W', 'JWST/NIRCam.F150W', 'JWST/NIRCam.F200W', 'JWST/NIRCam.F277W', 'JWST/NIRCam.F335M', 'JWST/NIRCam.F356W', 'JWST/NIRCam.F410M', 'JWST/NIRCam.F444W']\n",
      "2025-11-06 17:45:16,465 | synference | INFO     | Simulator recreated from grid at ../example_models/bpass_db_v4/grid_BPASS_Chab_DenseBasis_SFH_0.01_z_14_logN_2.7_Calzetti_v3_multinode.hdf5.\n",
      "2025-11-06 17:45:16,467 | synference | INFO     | Auto applying inverse log10 transform for log10_Av.\n",
      "2025-11-06 17:45:16,467 | synference | INFO     | Auto applying inverse log10 transform for log10_mass_weighted_age.\n",
      "2025-11-06 17:45:16,468 | synference | INFO     | Auto applying inverse log10 transform for log10_floor_sfr_10.\n",
      "2025-11-06 17:45:16,469 | synference | INFO     | Adding Av to tau_v transform.\n"
     ]
    }
   ],
   "source": [
    "fitter.recreate_simulator_from_library(override_library_path=library_path,\n",
    "                                        override_grid_path='test_grid.hdf5');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3cfee03",
   "metadata": {},
   "source": [
    "Then we can proceed to fit the observation using the sampler.\n",
    "\n",
    "The code will attempt to remove parameters which don't affect the fit (e.g. supplemental parameters) before fitting, but you can also specify to remove specific parameters using the `remove_params` argument.\n",
    "\n",
    "Let's choose an observation from the validation set and fit it using the dynesty sampler.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d1d70c60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[24.33915  21.76936  28.048773 28.16009  28.237339 28.1432   28.001326\n",
      " 28.250029]\n"
     ]
    }
   ],
   "source": [
    "index = 2\n",
    "\n",
    "data = '../example_models/bpass_db_v4/sbi_test_data_BPASS_DenseBasis_v4_final.npz'\n",
    "import numpy as np\n",
    "loaded = np.load(data)\n",
    "X_test = loaded['X'][:, index]\n",
    "y_test = loaded['y'][:, index]\n",
    "\n",
    "print(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b265a20d",
   "metadata": {},
   "source": [
    "We'll just recreate our prior for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6b9d5b14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-11-06 17:52:04,909 | synference | INFO     | ---------------------------------------------\n",
      "2025-11-06 17:52:04,914 | synference | INFO     | Prior ranges:\n",
      "2025-11-06 17:52:04,916 | synference | INFO     | ---------------------------------------------\n",
      "2025-11-06 17:52:04,918 | synference | INFO     | log_mass: 4.43 - 11.83 [log10_Msun]\n",
      "2025-11-06 17:52:04,919 | synference | INFO     | log10metallicity: -3.96 - -1.84 [log10(Zmet)]\n",
      "2025-11-06 17:52:04,920 | synference | INFO     | log10_Av: -2.88 - 0.59 [log10(mag)]\n",
      "2025-11-06 17:52:04,922 | synference | INFO     | log_sfr: -6.10 - 2.98 [log10(Msun/yr)]\n",
      "2025-11-06 17:52:04,923 | synference | INFO     | sfh_quantile_25: 0.16 - 0.77 [dimensionless]\n",
      "2025-11-06 17:52:04,926 | synference | INFO     | sfh_quantile_50: 0.23 - 0.88 [dimensionless]\n",
      "2025-11-06 17:52:04,927 | synference | INFO     | sfh_quantile_75: 0.39 - 0.98 [dimensionless]\n",
      "2025-11-06 17:52:04,928 | synference | INFO     | log10_mass_weighted_age: 2.16 - 2.91 [log10(Myr)]\n",
      "2025-11-06 17:52:04,929 | synference | INFO     | log10_floor_sfr_10: -6.00 - 2.95 [log10_floor(Msun/yr)]\n",
      "2025-11-06 17:52:04,930 | synference | INFO     | log_surviving_mass: 4.23 - 11.67 [log10_Msun]\n",
      "2025-11-06 17:52:04,931 | synference | INFO     | beta: -2.38 - 2.34 [dimensionless]\n",
      "2025-11-06 17:52:04,932 | synference | INFO     | ---------------------------------------------\n",
      "2025-11-06 17:52:04,991 | synference | INFO     | Processing prior...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Independent(CustomIndependentUniform(low: torch.Size([11]), high: torch.Size([11])), 1)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fitter.create_priors(set_self=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bbdd482",
   "metadata": {},
   "source": [
    "No we can run the sampler to fit the observation. This is too computationally expensive to run here, so we'll just show the code you would use."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5a49323",
   "metadata": {},
   "source": [
    "```python\n",
    "result = fitter.fit_observation_using_sampler(\n",
    "    observation=X_test,\n",
    "    sampler=\"dynesty\",\n",
    "    sampler_kwargs={\"bound\": \"multi\", \"sample\": \"rwalk\", \"run_kwargs\": {\"n_effective\": 2000}},\n",
    "    plot_name=f\"example_{index}\",\n",
    "    min_flux_pc_error=0.05,\n",
    ")\n",
    "samples = result[\"samples\"]\n",
    "log_l = result[\"logl\"]\n",
    "log_w = result[\"logwt\"]\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ecfb94b",
   "metadata": {},
   "source": [
    "We can sample using our trained SBI model as well for comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "63a854df",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling from posterior:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-11-06 17:52:21,385 | synference | ERROR    | Error occurred while sampling for sample 0: The trailing dimensions of `theta_or_x` do not match the `event_shape`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling from posterior: 100%|██████████| 1/1 [00:00<00:00, 17.12it/s]\n"
     ]
    }
   ],
   "source": [
    "predicted_params = fitter.sample_posterior(X_test, num_samples=1000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "astro",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
