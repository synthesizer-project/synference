{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Catalogue Fitting\n",
    "\n",
    "Synference has a method specifically for fitting catalogues of sources - `fit_catalogue`.\n",
    "\n",
    "This is designed to be a flexible and fast way to fit your full catalogue.\n",
    "\n",
    "It can handle:\n",
    "1. Transforming observations to match the expected model features.\n",
    "2. Prior predictive checks/outlier detection to remove sources that are unlikely to be well modelled.\n",
    "3. Optional imputation of missing data.\n",
    "4. Rapid posterior inference using trained model.\n",
    "5. Optional SED recovery (see [this notebook](sed_recovery.ipynb) for more details).\n",
    "6. Returning structured results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "from IPython.display import display\n",
    "from synthesizer import get_grids_dir\n",
    "from unyt import Jy\n",
    "\n",
    "from synference import SBI_Fitter, load_unc_model_from_hdf5, test_data_dir\n",
    "\n",
    "print(get_grids_dir())\n",
    "\n",
    "available_grids = os.listdir(get_grids_dir())\n",
    "if \"test_grid.hdf5\" not in available_grids:\n",
    "    cmd = f\"synthesizer-download --test-grids --destination {get_grids_dir()}\"\n",
    "    os.system(cmd)\n",
    "\n",
    "library_path = os.path.join(get_grids_dir(), \"test_grid.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "?SBI_Fitter.fit_catalogue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use a subset of the JADES spectroscopic catalogue used in the synference paper for this example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from astropy.table import Table\n",
    "\n",
    "cat = Table.read(f\"{test_data_dir}/jades_spec_catalogue_subset.fits\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step is the same as our SED recovery example - we will load our trained model and the noise model used to train the model. We need the noise model to transform our observations to match the model features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "library_path = (\n",
    "    f\"{test_data_dir}/grid_BPASS_Chab_DenseBasis_SFH_0.01_z_14_logN_2.7_Calzetti_v3_multinode.hdf5\"  # noqa: E501\n",
    ")\n",
    "\n",
    "fitter = SBI_Fitter.load_saved_model(\n",
    "    model_file=f\"{test_data_dir}\", library_path=library_path, device=\"cpu\"\n",
    ")\n",
    "\n",
    "nm_path = f\"{test_data_dir}/BPASS_DenseBasis_v4_final_nsf_0_params_empirical_noise_models.h5\"\n",
    "noise_models = load_unc_model_from_hdf5(nm_path)\n",
    "\n",
    "fitter.feature_array_flags[\"empirical_noise_models\"] = noise_models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to explain our catalogue to synference. We create a 'conversion_dict' dictionary which maps the feature names to columns in the table.\n",
    "**The expected column names for flux are the SVO format filter names, e.g. JWST/NIRCam.F070W, or just F070W if it is unambiguous.**\n",
    "**For flux errors it is the same, but with 'unc_' prefixed, e.g. unc_JWST/NIRCam.F070W or unc_F070W.**\n",
    "**Any other features (e.g. redshift) should be mapped to the column name in the catalogue.**\n",
    "\n",
    "\n",
    "In this case our catalogue is almost in the correct format, so we just need to convert the band names to include the facility and instrument. We do this by getting the band names from the fitted model (`fitter.feature_names`) and replacing the relevant parts of the strings.\n",
    "\n",
    "We then create our conversion dictionary to map from e.g. 'unc_F200W' to 'unc_JWST/NIRCam.F200W'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def band_to_instrument(band):\n",
    "    \"\"\"Band name to instrument mapping.\"\"\"\n",
    "    if band in [\"F435W\", \"F606W\", \"F775W\", \"F814W\", \"F850LP\"]:\n",
    "        return f\"HST/ACS_WFC.{band}\"\n",
    "    return f\"JWST/NIRCam.{band}\"\n",
    "\n",
    "\n",
    "bands = [\n",
    "    i.split(\".\")[-1]\n",
    "    for i in fitter.feature_names\n",
    "    if not (i.startswith(\"unc_\") or i.startswith(\"redshift\"))\n",
    "]\n",
    "\n",
    "print(bands)\n",
    "\n",
    "conversion_dict = {band: band_to_instrument(band) for band in bands}\n",
    "conversion_dict.update({f\"unc_{band}\": f\"unc_{band_to_instrument(band)}\" for band in bands})\n",
    "conversion_dict[\"redshift\"] = \"redshift\"\n",
    "\n",
    "conversion_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can run the inference. We specify the catalogue table, the conversion dictionary, the input flux units (Jy), and then we have some choices.\n",
    "\n",
    "We can choose to:\n",
    "\n",
    "1. Run predictive checks to remove outliers. \n",
    "2. Impute or remove missing data.\n",
    "3. Run posterior inference, setting the number of posterior samples to draw.    \n",
    "4. Recover SEDs for each source."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fitter.recreate_simulator_from_library(\n",
    "    override_library_path=library_path, override_grid_path=\"test_grid.hdf5\"\n",
    ")\n",
    "\n",
    "post_tab = fitter.fit_catalogue(\n",
    "    cat,\n",
    "    columns_to_feature_names=conversion_dict,\n",
    "    flux_units=Jy,\n",
    "    check_out_of_distribution=False,\n",
    "    recover_SEDs=False,\n",
    "    missing_data_flag=np.nan,\n",
    "    num_samples=300,\n",
    "    append_to_input=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can look at the output table, which is also an `astropy.table.Table`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "post_tab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can plot the star-forming main sequence from the fitted catalogue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.scatter(post_tab[\"log_mass_50\"], post_tab[\"log_sfr_50\"])\n",
    "plt.xlabel(\"Stellar Mass (log M_sun)\")\n",
    "plt.ylabel(\"SFR (log M_sun/yr)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may notice some of rows and nans. This is because those rows had missing data that we chose not to impute, which can't be handled by the SBI inference method.\n",
    "\n",
    "We could set `missing_data_mcmc` to `True` to impute those missing values instead."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also recover and plot the SEDs, which we'll do for the first few sources in the catalogue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "post_tab, data = fitter.fit_catalogue(\n",
    "    cat[:4],\n",
    "    columns_to_feature_names=conversion_dict,\n",
    "    flux_units=Jy,\n",
    "    check_out_of_distribution=False,\n",
    "    recover_SEDs=True,\n",
    "    plot_SEDs=True,\n",
    "    missing_data_flag=np.nan,\n",
    "    num_samples=300,\n",
    "    append_to_input=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you recover the SEDs then a nested dictionary with the SED posteriors are also returned. Initial key is the source index or ID, then within that there are keys for 'wav', 'fnu_quantiles', 'wav', 'phot_wav', 'phot_fnu_draws' and 'fig'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[1].keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the SED plots:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in data.keys():\n",
    "    display(data[key][\"fig\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check the feature array used for inference by setting `return_feature_array=True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_array, mask = post_tab, data = fitter.fit_catalogue(\n",
    "    cat,\n",
    "    columns_to_feature_names=conversion_dict,\n",
    "    flux_units=Jy,\n",
    "    check_out_of_distribution=False,\n",
    "    return_feature_array=True,\n",
    "    missing_data_flag=np.nan,\n",
    "    num_samples=300,\n",
    "    append_to_input=False,\n",
    ")\n",
    "\n",
    "feature_array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also run prior predictive checks to identify outliers, by setting `check_out_of_distribution=True`. The available list of methods is any of those in PYOD (https://pyod.readthedocs.io/en/latest/). You can set them with the 'outlier_methods' argument as a list of strings."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}
