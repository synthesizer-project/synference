{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1ef5c0c7",
   "metadata": {},
   "source": [
    "# Simulation-Based Inference: A Practical Introduction\n",
    "\n",
    "## 1. Introduction and Motivation\n",
    "\n",
    "Simulation-based inference (SBI), also known as likelihood-free inference or implicit likelihood inference, addresses a fundamental challenge in modern scientific modeling: **what do we do when we can simulate data from our model but cannot evaluate the likelihood?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bae33313",
   "metadata": {},
   "source": [
    "\n",
    "In traditional Bayesian inference, we want to compute the posterior:\n",
    "\n",
    "$$p(\\theta | x) = \\frac{p(x | \\theta) p(\\theta)}{p(x)}$$\n",
    "\n",
    "This requires:\n",
    "- A prior $p(\\theta)$\n",
    "- A likelihood $p(x | \\theta)$ that we can evaluate\n",
    "- Computing the evidence $p(x) = \\int p(x | \\theta) p(\\theta) d\\theta$\n",
    "\n",
    "**However**, in many scientific domains (astrophysics, cosmology, neuroscience, ecology, epidemiology), we have:\n",
    "- ✅ A simulator that can generate data: $x \\sim p(x | \\theta)$\n",
    "- ✅ A prior over parameters: $\\theta \\sim p(\\theta)$\n",
    "- ❌ No tractable likelihood function $p(x | \\theta)$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96fab20e",
   "metadata": {},
   "source": [
    "## 2. The Core Idea of SBI\n",
    "\n",
    "The key insight: **If we can simulate, we can learn the statistical relationships we need using machine learning.**\n",
    "\n",
    "Instead of deriving likelihoods analytically, we:\n",
    "1. Generate many simulations: $(θ_i, x_i)$ where $\\theta_i \\sim p(\\theta)$, $x_i \\sim p(x|\\theta_i)$\n",
    "2. Use neural networks to learn probability distributions from these samples\n",
    "3. Apply these learned distributions to perform inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bb12561",
   "metadata": {},
   "source": [
    "### Three Main Approaches\n",
    "\n",
    "| Method | What it learns | Target Distribution |\n",
    "|--------|---------------|-------------------|\n",
    "| **Neural Posterior Estimation (NPE)** | $p(\\theta \\| x)$ directly | Posterior |\n",
    "| **Neural Likelihood Estimation (NLE)** | $p(x \\| \\theta)$ | Likelihood |\n",
    "| **Neural Ratio Estimation (NRE)** | $r(x,\\theta) = \\frac{p(x\\|\\theta)}{p(x)}$ | Likelihood ratio |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c223232",
   "metadata": {},
   "source": [
    "\n",
    "## 3. Mathematical Framework\n",
    "\n",
    "### 3.1 The Joint Distribution\n",
    "\n",
    "Start with the joint distribution over parameters and data:\n",
    "\n",
    "$$p(\\theta, x) = p(x | \\theta) p(\\theta)$$\n",
    "\n",
    "From simulation, we can sample from this joint distribution:\n",
    "- Sample $\\theta \\sim p(\\theta)$\n",
    "- Sample $x \\sim p(x|\\theta)$ using the simulator\n",
    "\n",
    "This gives us a dataset: $\\mathcal{D} = \\{(\\theta_i, x_i)\\}_{i=1}^N$\n",
    "\n",
    "### 3.2 Neural Posterior Estimation (NPE)\n",
    "\n",
    "**Goal**: Approximate $p(\\theta | x)$ directly using a neural network.\n",
    "\n",
    "**Method**: Train a conditional density estimator $q_\\phi(\\theta | x)$ to minimize:\n",
    "\n",
    "$$\\mathcal{L}_{\\text{NPE}}(\\phi) = -\\mathbb{E}_{(\\theta, x) \\sim p(\\theta, x)}[\\log q_\\phi(\\theta | x)]$$\n",
    "\n",
    "This is the **negative log-likelihood** of the training data under our neural density estimator.\n",
    "\n",
    "**Key equation**: \n",
    "$$q_\\phi^*(\\theta | x) \\approx p(\\theta | x)$$\n",
    "\n",
    "where $\\phi^*$ are the optimal parameters.\n",
    "\n",
    "### 3.3 Neural Likelihood Estimation (NLE)\n",
    "\n",
    "**Goal**: Approximate $p(x | \\theta)$ using a neural network.\n",
    "\n",
    "**Method**: Train a conditional density estimator $q_\\phi(x | \\theta)$ to minimize:\n",
    "\n",
    "$$\\mathcal{L}_{\\text{NLE}}(\\phi) = -\\mathbb{E}_{(\\theta, x) \\sim p(\\theta, x)}[\\log q_\\phi(x | \\theta)]$$\n",
    "\n",
    "**Key equations**: Once we have $q_\\phi(x | \\theta) \\approx p(x | \\theta)$, we can use it in MCMC or SMC to sample from:\n",
    "\n",
    "$$p(\\theta | x_o) \\propto q_\\phi(x_o | \\theta) p(\\theta)$$\n",
    "\n",
    "### 3.4 Neural Ratio Estimation (NRE)\n",
    "\n",
    "**Goal**: Approximate the likelihood-to-evidence ratio.\n",
    "\n",
    "**Method**: This uses a clever binary classification trick. Define:\n",
    "\n",
    "$$r(x, \\theta) = \\frac{p(x | \\theta)}{p(x)} = \\frac{p(x, \\theta)}{p(\\theta)p(x)}$$\n",
    "\n",
    "Train a binary classifier $d_\\phi(x, \\theta)$ to distinguish between:\n",
    "- Positive class: $(x, \\theta)$ drawn from joint $p(x, \\theta) = p(x|\\theta)p(\\theta)$\n",
    "- Negative class: $(x, \\theta)$ drawn from marginals $p(x)p(\\theta)$\n",
    "\n",
    "**Key equation**: The optimal classifier satisfies:\n",
    "\n",
    "$$d_\\phi^*(x, \\theta) = \\frac{p(x, \\theta)}{p(x, \\theta) + p(x)p(\\theta)} = \\frac{r(x, \\theta)}{1 + r(x, \\theta)}$$\n",
    "\n",
    "Therefore:\n",
    "$$r(x, \\theta) = \\frac{d_\\phi^*(x, \\theta)}{1 - d_\\phi^*(x, \\theta)}$$\n",
    "\n",
    "And we can compute:\n",
    "$$p(\\theta | x_o) \\propto r(x_o, \\theta) p(\\theta) = \\frac{d_\\phi(x_o, \\theta)}{1 - d_\\phi(x_o, \\theta)} p(\\theta)$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1d5c546",
   "metadata": {},
   "source": [
    "\n",
    "## 4. Neural Density Estimators\n",
    "\n",
    "The key to SBI is using flexible neural networks to represent probability distributions. Here are the most common architectures:\n",
    "\n",
    "### 4.1 Mixture Density Networks (MDN)\n",
    "\n",
    "A neural network outputs the parameters of a mixture distribution:\n",
    "\n",
    "$$q_\\phi(\\theta | x) = \\sum_{k=1}^K \\pi_k(x) \\mathcal{N}(\\theta | \\mu_k(x), \\Sigma_k(x))$$\n",
    "\n",
    "where a neural network $f_\\phi(x)$ outputs $\\{\\pi_k, \\mu_k, \\Sigma_k\\}$ for each mixture component.\n",
    "\n",
    "**Advantages**: \n",
    "- Can represent multimodal distributions\n",
    "- Relatively simple to implement\n",
    "\n",
    "**Disadvantages**:\n",
    "- Limited flexibility\n",
    "- Difficult to scale to high dimensions\n",
    "\n",
    "### 4.2 Normalizing Flows\n",
    "\n",
    "A normalizing flow transforms a simple base distribution through a series of invertible transformations:\n",
    "\n",
    "$$\\theta = T_\\phi(z, x), \\quad z \\sim p(z)$$\n",
    "\n",
    "The density is given by the change of variables formula:\n",
    "\n",
    "$$q_\\phi(\\theta | x) = p(z) \\left| \\det \\frac{\\partial T_\\phi^{-1}(\\theta, x)}{\\partial \\theta} \\right|$$\n",
    "\n",
    "**Common flow architectures**:\n",
    "- **MAF (Masked Autoregressive Flow)**: Good for density estimation (NPE)\n",
    "- **IAF (Inverse Autoregressive Flow)**: Good for sampling\n",
    "- **Neural Spline Flows**: Uses monotonic splines for transformations\n",
    "- **Continuous Normalizing Flows**: Uses neural ODEs\n",
    "\n",
    "**Advantages**:\n",
    "- Exact density evaluation\n",
    "- Exact sampling\n",
    "- Very flexible\n",
    "\n",
    "**Disadvantages**:\n",
    "- Can be computationally expensive\n",
    "- Architecture design is important\n",
    "\n",
    "### 4.3 Autoregressive Models\n",
    "\n",
    "Decompose the joint density using the chain rule:\n",
    "\n",
    "$$q_\\phi(\\theta | x) = \\prod_{i=1}^d q_\\phi(\\theta_i | \\theta_{<i}, x)$$\n",
    "\n",
    "Each conditional is modeled by a neural network.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34bf1f44",
   "metadata": {},
   "source": [
    "\n",
    "## 5. Sequential vs. Amortized Inference\n",
    "\n",
    "### 5.1 Amortized Inference\n",
    "\n",
    "**Single round**: Train once on many simulations, then apply to any observation.\n",
    "\n",
    "$$(\\theta_i, x_i) \\sim p(\\theta, x) \\text{ for } i=1,\\ldots,N$$\n",
    "\n",
    "Train $q_\\phi(\\theta | x)$ on this dataset. Then for any new observation $x_o$, directly evaluate $q_\\phi(\\theta | x_o)$.\n",
    "\n",
    "**Advantages**: Very fast at inference time  \n",
    "**Disadvantages**: May be inaccurate far from training distribution\n",
    "\n",
    "### 5.2 Sequential Inference\n",
    "\n",
    "**Multiple rounds**: Iteratively focus simulations near observed data.\n",
    "\n",
    "**Round 1**: Sample from prior, train initial approximation $q_\\phi^{(1)}$\n",
    "\n",
    "**Round t**: \n",
    "- Sample $\\theta \\sim q_\\phi^{(t-1)}(\\cdot | x_o)$ (focus near posterior)\n",
    "- Simulate $x \\sim p(x|\\theta)$\n",
    "- Retrain to get $q_\\phi^{(t)}$\n",
    "\n",
    "**Advantages**: Much more simulation-efficient, better accuracy  \n",
    "**Disadvantages**: Must re-simulate for each new observation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df1768b1",
   "metadata": {},
   "source": [
    "\n",
    "## 6. Comparison with Traditional Methods\n",
    "\n",
    "| Method | Requires Likelihood | Flexibility | Simulation Efficiency |\n",
    "|--------|-------------------|-------------|----------------------|\n",
    "| **MCMC** | Yes | High | N/A |\n",
    "| **ABC** | No | Medium | Low (many rejections) |\n",
    "| **NPE** | No | High | High (amortized) |\n",
    "| **NLE + MCMC** | No | High | Medium |\n",
    "| **NRE** | No | High | Medium |\n",
    "\n",
    "### Key Advantages of SBI:\n",
    "1. **Works with implicit likelihoods**: No need to derive complex probability densities\n",
    "2. **Amortization**: Train once, apply to many observations (for NPE)\n",
    "3. **Scalability**: Modern neural networks handle high-dimensional data\n",
    "4. **Automatic**: No hand-crafted distance metrics (unlike ABC)\n",
    "\n",
    "### Challenges:\n",
    "1. **Diagnostics**: Harder to assess convergence and accuracy\n",
    "2. **Simulation cost**: Still needs many simulations for training\n",
    "3. **Extrapolation**: Networks may perform poorly outside training distribution\n",
    "4. **Stochastic simulators**: Need special handling for variance\n",
    "\n",
    "## 7. Practical Considerations\n",
    "\n",
    "### 7.1 Summary Statistics vs. Raw Data\n",
    "\n",
    "For high-dimensional data, often use summary statistics:\n",
    "- Reduces dimensionality\n",
    "- Focuses on relevant features\n",
    "- **Caution**: May lose information needed for inference\n",
    "\n",
    "### 7.2 Model Misspecification\n",
    "\n",
    "What if the simulator doesn't match reality?\n",
    "\n",
    "- SBI learns $p(\\theta | x, \\mathcal{M})$ where $\\mathcal{M}$ is your model\n",
    "- Garbage in, garbage out: poor simulators give poor inference\n",
    "- Consider **model criticism** and **posterior predictive checks**\n",
    "\n",
    "### 7.3 Computational Cost\n",
    "\n",
    "Training neural networks requires:\n",
    "- Many simulations (thousands to millions)\n",
    "- GPU compute for training\n",
    "- Careful hyperparameter tuning\n",
    "\n",
    "**But**: Once trained, inference is very fast!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d174563",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**Key Papers**:\n",
    "- Papamakarios & Murray (2016) - Fast ε-free Inference of Simulation Models with Bayesian Conditional Density Estimation\n",
    "- Greenberg et al. (2019) - Automatic Posterior Transformation for Likelihood-Free Inference\n",
    "- Cranmer et al. (2020) - The frontier of simulation-based inference\n",
    "\n",
    "**Software**:\n",
    "- `sbi`: Python package for SBI (https://sbi-dev.github.io/sbi/)\n",
    "- `lampe`: Likelihood-free AMortized Posterior Estimation\n",
    "- `sbibm`: SBI benchmarking framework\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
