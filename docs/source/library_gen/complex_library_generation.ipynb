{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b420dbed",
   "metadata": {},
   "source": [
    "# Complex Library Generation\n",
    "\n",
    "In addition to the basic library generation demonstrated previously, Synference also supports more complex scenarios, such as grids with more supplementary parameters, custom observation transformations, and more complicated model setups. Below is an example of how to generate a more complex model library using Synference.\n",
    "\n",
    "\n",
    "Firstly we'll just import the necessary modules and set up the synthesizer model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c57bea49",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from astropy.cosmology import Planck18 as cosmo\n",
    "from synthesizer.emission_models.attenuation import (\n",
    "    Calzetti2000,\n",
    ")  # noqa\n",
    "from synthesizer.emission_models.dust.emission import Greybody, IR_templates  # noqa\n",
    "from synthesizer.emission_models.stellar.pacman_model import (\n",
    "    PacmanEmission,\n",
    ")  # noqa\n",
    "from synthesizer.grid import Grid\n",
    "from synthesizer.instruments import FilterCollection, Instrument\n",
    "from synthesizer.parametric import ZDist\n",
    "from tqdm import tqdm\n",
    "from unyt import K"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "859bb90c",
   "metadata": {},
   "source": [
    "And our Synference components - the GalaxyBasis class, and some utility functions we will use later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2fe82d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "from synference import (\n",
    "    GalaxyBasis,\n",
    "    calculate_balmer_decrement,\n",
    "    calculate_beta,\n",
    "    calculate_colour,\n",
    "    calculate_d4000,\n",
    "    calculate_mass_weighted_age,\n",
    "    calculate_muv,\n",
    "    calculate_sfh_quantile,\n",
    "    calculate_surviving_mass,\n",
    "    draw_from_hypercube,\n",
    "    generate_random_DB_sfh,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70cb723f",
   "metadata": {},
   "source": [
    "Firstly we'll set up our Synthesizer similarly to before - see the basic library generation example for more details.\n",
    "\n",
    "For this example we'll use a set of filter used in wide area surveys, including VISTA, Subaru Hyper Suprime-Cam, Euclid, and Spitzer IRAC.\n",
    "\n",
    "Note that if you're running this step on a cluster node without internet access, you'll need to create the instrument file beforehand and pass in the HDF5 file path insead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cfc44396",
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_codes = [\n",
    "    \"Paranal/VISTA.Z\",\n",
    "    \"Paranal/VISTA.Y\",\n",
    "    \"Paranal/VISTA.J\",\n",
    "    \"Paranal/VISTA.H\",\n",
    "    \"Paranal/VISTA.Ks\",\n",
    "    \"Subaru/HSC.g\",\n",
    "    \"Subaru/HSC.r\",\n",
    "    \"Subaru/HSC.i\",\n",
    "    \"Subaru/HSC.z\",\n",
    "    \"Subaru/HSC.Y\",\n",
    "    \"CFHT/MegaCam.u\",\n",
    "    \"CFHT/MegaCam.g\",\n",
    "    \"CFHT/MegaCam.r\",\n",
    "    \"CFHT/MegaCam.i\",\n",
    "    \"CFHT/MegaCam.z\",\n",
    "    \"Euclid/VIS.vis\",\n",
    "    \"Euclid/NISP.Y\",\n",
    "    \"Euclid/NISP.J\",\n",
    "    \"Euclid/NISP.H\",\n",
    "    \"Spitzer/IRAC.I1\",\n",
    "    \"Spitzer/IRAC.I2\",\n",
    "]\n",
    "\n",
    "filterset = FilterCollection(filter_codes=filter_codes)\n",
    "\n",
    "instrument = Instrument(\"EuclidDeep\", filters=filterset)\n",
    "\n",
    "grid = Grid(\"test_grid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8593eca3",
   "metadata": {},
   "source": [
    "Now we can configure our model, which we will make more complex. Firstly we define the prior ranges for our main galaxy parameters.\n",
    "\n",
    "This model will use a more complex dust attenuation model, with variable UV slope and 2175A bump strength, as well as a variable escape fraction for ionizing photons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9b6f21d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_models = 1000\n",
    "\n",
    "redshift = (0.01, 14)\n",
    "masses = (4, 12)  # log(M/Msun)=4 to log(M/Msun)=12\n",
    "logAv = (-3, 0.7)  # Av=0.001 to Av=5\n",
    "log_zmet = (-4, -1.39)  # Z=0.0001 to Z=0.04\n",
    "fesc = (0.0, 1.0)  # Fraction of ionizing photons that escape the galaxy\n",
    "slope = (-0.4, 1.1)  # UV slope modification to Calzetti law\n",
    "bump_strength = (0.0, 3.0)  # 2175A bump strength in Calzetti law\n",
    "\n",
    "\n",
    "prior_ranges = {\n",
    "    \"redshift\": redshift,\n",
    "    \"log_masses\": masses,\n",
    "    \"log_Av\": logAv,  # Av in magnitudes\n",
    "    \"log_zmet\": log_zmet,\n",
    "    \"fesc\": fesc,\n",
    "    \"slope\": slope,\n",
    "    \"bump_strength\": bump_strength,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8baabcb9",
   "metadata": {},
   "source": [
    "### Star Formation History\n",
    "\n",
    "We'll use a non-parametric 'Dense Basis' SFH (Iyer et al. 2019), where we model the time at which different quantiles of stellar mass formed. Synference provides a helper module for generating these Star Formation Histories.\n",
    "\n",
    "We will use 3 quantiles, e.g. $t_{25}, t_{50}, t_{75}$, so we will simply add three dummy parameters to our LHC sampling of the parameter space, and then replace them afterward. We need to set the concentration parameter $\\alpha$ for the Dirichilet prior on the SFH, for which we will use $\\alpha=3$. This controls the correlation between different SFH quantiles, where lower values will have more rapdily varying star formation histories. We also set a prior on the recent SFR, in terms of the sSFR, which normalizes by the stellar mass. This allows for a range from quiescent to highly star-forming galaxies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c081244d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tx_alpha = 3\n",
    "\n",
    "for i in range(3):\n",
    "    j = 100 * (i + 1) / (4)\n",
    "    prior_ranges[f\"sfh_quantile_{j:.0f}\"] = (0, 1)\n",
    "\n",
    "prior_ranges[\"ssfr\"] = (-14, -7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2f71bbe",
   "metadata": {},
   "source": [
    "Now we will sample these parameters from our hypercube. Note that we set a log prior on dust attenuation $A_V$, but we want to sample linear dust attenuation, so we get the model to 'unlog' the parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2f9938b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_param_dict = draw_from_hypercube(prior_ranges, N_models, unlog_keys=[\"log_Av\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd2afc37",
   "metadata": {},
   "source": [
    "Now we will create our metallicity distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f6207629",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating ZDist: 100%|██████████| 1000/1000 [00:00<00:00, 321451.87it/s]\n"
     ]
    }
   ],
   "source": [
    "Z_dists = [\n",
    "    ZDist.DeltaConstant(log10metallicity=log_z)\n",
    "    for log_z in tqdm(all_param_dict[\"log_zmet\"], desc=\"Creating ZDist\")\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "640cbfd6",
   "metadata": {},
   "source": [
    "Now we can create the star-formation history. We are using a specific prior in specific star-formation rate here, but we need to provide log SFR to the function, so we calculate this inside the loop. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6df824bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating SFH models: 100%|██████████| 1000/1000 [00:01<00:00, 679.82it/s]\n"
     ]
    }
   ],
   "source": [
    "# Draw SFH params from prior\n",
    "sfh_models = []\n",
    "for i in tqdm(range(N_models), desc=\"Generating SFH models\"):\n",
    "    z = all_param_dict[\"redshift\"][i]\n",
    "    logmass = all_param_dict[\"log_masses\"][i]\n",
    "    logssfr = all_param_dict[\"ssfr\"][i]\n",
    "    logsfr = logmass + logssfr\n",
    "    sfh, tx = generate_random_DB_sfh(\n",
    "        Nparam=3,\n",
    "        tx_alpha=tx_alpha,\n",
    "        redshift=z,\n",
    "        logsfr=logsfr,\n",
    "        logmass=logmass,\n",
    "    )\n",
    "    for j in range(3):\n",
    "        all_param_dict[f\"sfh_quantile_{100 * (j + 1) / (3 + 1):.0f}\"][i] = tx[j]\n",
    "    sfh_models.append(sfh)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33192b06",
   "metadata": {},
   "source": [
    "Now we will set up two functions, which are used to convert parameters from one form to another inside the model, and allow the SFH model to be serialized and re-created. This is a flexible system which should allow complex transformations.\n",
    "\n",
    "These are required because synference automatically looks for varying parameters which are stored on the Synthesizer galaxies and emitters, but if they are in a complex form (e.g. the Dense Basis tuple), then we need to explain how to understand them to the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "93f3a653",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_db_tuple(params):\n",
    "    \"\"\"Constructs the DenseBasis tuple from the SFH.\"\"\"\n",
    "    nquant = 0\n",
    "    for key in params:\n",
    "        if key.startswith(\"sfh_quantile_\"):\n",
    "            nquant += 1\n",
    "\n",
    "    mass_quantiles = np.linspace(0, 1, nquant + 2)[1:-1]  # Exclude the 0 and 1 quantiles\n",
    "\n",
    "    db_tuple = [params[\"log_mass\"], params[\"log_sfr\"], nquant] + [\n",
    "        params[f\"sfh_quantile_{int(q * 100)}\"] for q in mass_quantiles\n",
    "    ]\n",
    "    return db_tuple  # Return a tuple of (log_mass, SFR, nquant, [quantiles...])\n",
    "\n",
    "\n",
    "def db_sf_convert(param, param_dict, Nparam_SFH=3):\n",
    "    \"\"\"Converts from a DenseBasis tuple back to parameters.\"\"\"\n",
    "    db_tuple = param_dict[\"db_tuple\"]\n",
    "    # dp_tuple has the folliwng\n",
    "    # mass, sfr, tx_alpha, *sfh_quantiles\n",
    "    if param.startswith(\"sfh_quantile_\"):\n",
    "        # Convert the SFH quantile parameters to the Dense Basis SFH format\n",
    "        j = int(np.round(int(param.split(\"_\")[-1]) / 100 * (Nparam_SFH + 1)))\n",
    "        return db_tuple[j + 2]  # +3 because first three are mass, sfr, tx_alpha\n",
    "    elif param == \"log_sfr\":\n",
    "        # Convert log_sfr to the Dense Basis SFH format\n",
    "        return db_tuple[1]\n",
    "    elif param == \"log_masses\":\n",
    "        # Convert log_masses to the Dense Basis SFH format\n",
    "        return db_tuple[0]\n",
    "    elif param == \"tx_alpha\":\n",
    "        # Convert tx_alpha to the Dense Basis SFH format\n",
    "        return db_tuple[2]\n",
    "    elif param == \"log_ssfr\":\n",
    "        # Convert log_ssfr to the Dense Basis SFH format\n",
    "        return db_tuple[1] - db_tuple[0]\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown parameter {param.str} in db_tuple conversion.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87d92a48",
   "metadata": {},
   "source": [
    "Now we can set up some parameter transformations. These are for when we want to sample a parameter which is not what is used directly in Synthesizer. The simplest example is our use of the V-band attenuation $A_V$, whereas in reality to generate a model we must provide the V-band optical depth $\\tau_V$, to Synthesizer. So we provide a function to Synference to allow it to do this conversion, which is simply $A_V = \\tau_V * 1.086$\n",
    "\n",
    "These generally take the form of a dictionary, where the key is the parameter name required by the Synthesizer model, and value is a two component tuple, where the first value is a new name (or a list of new names, for multiple parameters), and the second value the conversion function.\n",
    "\n",
    "We will also save the inverse functions in a separate dictionary. These are only used if we want to recreate this simulator later, to generate SEDs from a set of input parameters. This is useful to recover SEDs for observations when performing inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3302328f",
   "metadata": {},
   "outputs": [],
   "source": [
    "alt_parametrizations = {\n",
    "    \"tau_v\": (\"Av\", lambda x: x[\"tau_v\"] * 1.086),\n",
    "    \"db_tuple\": (\n",
    "        [\"log_sfr\"] + [f\"sfh_quantile_{100 * (j + 1) / (3 + 1):.0f}\" for j in range(3)],\n",
    "        db_sf_convert,\n",
    "    ),\n",
    "}\n",
    "\n",
    "param_transforms_to_save = {\n",
    "    \"tau_v\": lambda x: x[\"Av\"] / 1.086,\n",
    "    \"db_tuple\": make_db_tuple,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "535aae99",
   "metadata": {},
   "source": [
    "### Emission Models\n",
    "\n",
    "Now we will set up our complex emission model which supports our priors, with variable escape fraction and flexible attenuation law.\n",
    "\n",
    "The basic concept is simple: **Any emission model parameter set with a string, rather than an explicit value, will be inherited from the emission model, or emitter.** So in this case, for 'tau_v', 'fesc', 'bump_strength', and 'slope', Synthesizer will look for these parameters to be set on the individual Galaxy or Star instances.\n",
    "\n",
    "We will also set the emission key we will save, which is the root of the emission model, named 'total'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9fb0b870",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating emission model.\n",
      "Root emission model label is: total\n"
     ]
    }
   ],
   "source": [
    "dust_emission = Greybody(temperature=40 * K, emissivity=1.5)\n",
    "dust_curve = Calzetti2000(slope=\"slope\", ampl=\"bump_strength\")\n",
    "\n",
    "print(\"Creating emission model.\")\n",
    "emission_model = PacmanEmission(\n",
    "    grid=grid, tau_v=\"tau_v\", dust_curve=dust_curve, dust_emission=dust_emission, fesc=\"fesc\"\n",
    ")\n",
    "\n",
    "emission_key = emission_model.label\n",
    "\n",
    "print(f\"Root emission model label is: {emission_key}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27558101",
   "metadata": {},
   "source": [
    "To ensure these parameters are set on each galaxy, we will create our final input, the ```galaxy_params``` dictionary. This is simply a dictionary of parameter name and value array pairs for every galaxy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "97a1d91e",
   "metadata": {},
   "outputs": [],
   "source": [
    "galaxy_params = {\n",
    "    \"fesc\": all_param_dict[\"fesc\"],\n",
    "    \"tau_v\": all_param_dict[\"Av\"] / 1.086,\n",
    "    \"bump_strength\": all_param_dict[\"bump_strength\"],\n",
    "    \"slope\": all_param_dict[\"slope\"],\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27812cf5",
   "metadata": {},
   "source": [
    "Now we can instantiate the GalaxyBasis, into which we will pass these inputs. This won't do much until we call the correct function to build the grid. Note that we set 'build_grid' = False, because we have already generated our full grid of parameters. If we wanted we could also pass in a smaller set of parameter values instead, and set build_grid=True, and the code would generate all the combinations of those parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c7cc6d0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-11-05 11:44:45,409 | synference | INFO     | Generating grid directly from provided parameter samples.\n"
     ]
    }
   ],
   "source": [
    "basis = GalaxyBasis(\n",
    "    model_name=\"sps_Euclid_test\",\n",
    "    redshifts=all_param_dict[\"redshift\"],\n",
    "    grid=grid,\n",
    "    emission_model=emission_model,\n",
    "    sfhs=sfh_models,\n",
    "    cosmo=cosmo,\n",
    "    instrument=instrument,\n",
    "    metal_dists=Z_dists,\n",
    "    galaxy_params=galaxy_params,\n",
    "    alt_parametrizations=alt_parametrizations,\n",
    "    redshift_dependent_sfh=True,\n",
    "    build_grid=False,\n",
    "    log_stellar_masses=all_param_dict[\"log_masses\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05cc557d",
   "metadata": {},
   "source": [
    "Something else we can do here to improve the utility of our model is add more parameters to be saved and stored. Synference provides a set of these parameters, to save things like the surviving stellar mass, UV magnitude, $\\beta$ slope, D4000 break strength, UVJ colors, etc. We can see the full list here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "aaad90a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Available supplementary functions:\n",
       "  - calculate_MUV\n",
       "  - calculate_Ndot_ion\n",
       "  - calculate_agn_fraction\n",
       "  - calculate_balmer_decrement\n",
       "  - calculate_beta\n",
       "  - calculate_colour\n",
       "  - calculate_d4000\n",
       "  - calculate_flux_weighted_age\n",
       "  - calculate_line_ew\n",
       "  - calculate_line_flux\n",
       "  - calculate_lum_weighted_age\n",
       "  - calculate_mass_weighted_age\n",
       "  - calculate_muv\n",
       "  - calculate_sfh_quantile\n",
       "  - calculate_sfr\n",
       "  - calculate_surviving_mass\n",
       "  - calculate_xi_ion0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from synference import SUPP_FUNCTIONS\n",
    "\n",
    "SUPP_FUNCTIONS()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48bc8091",
   "metadata": {},
   "source": [
    "We can pass in these functions to our ```GalaxyBasis.create_mock_library``` function, and they will be run for every galaxy and the output stored in the grid.\n",
    "\n",
    "The functions should take the galaxy as the first argument, and then the following arguments (if any) will be set by position as demonstrated below. The keys will be the parameter names. The return should be either a single value (float, string, unyt_quantity) or a dictionary which maps to the emission model names (e.g to record $\\beta$ slope for different components)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fa246302",
   "metadata": {},
   "outputs": [],
   "source": [
    "supp_params = {\n",
    "    \"mUV\": (calculate_muv, cosmo),\n",
    "    \"sfh_quant_25\": (calculate_sfh_quantile, 0.25, True),  # Calculate SFH quantile at 25%\n",
    "    \"sfh_quant_50\": (calculate_sfh_quantile, 0.50, True),  # Calculate SFH quantile at 50%\n",
    "    \"sfh_quant_75\": (calculate_sfh_quantile, 0.75, True),  # Calculate SFH quantile at 75%\n",
    "    \"UV\": (calculate_colour, \"U\", \"V\", emission_key, True),  # Calculate UV colour (rest-frame)\n",
    "    \"VJ\": (calculate_colour, \"V\", \"J\", emission_key, True),  # Calculate VJ colour (rest-frame)\n",
    "    \"log_surviving_mass\": (calculate_surviving_mass, grid),  # Calculate surviving mass\n",
    "    \"d4000\": (calculate_d4000, emission_key),  # Calculate D4000 using the emission model\n",
    "    \"beta\": (calculate_beta, emission_key),\n",
    "    \"balmer_decrement\": (calculate_balmer_decrement, emission_key),\n",
    "    \"mass_weighted_age\": calculate_mass_weighted_age,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7130ca9d",
   "metadata": {},
   "source": [
    "### Grid Generation\n",
    "\n",
    "Now we can run the final method to create the output catalogue. There are several things to note here. We can set the number of processes to use, to make use of multiple threads. We can also set the batch size, which will split the generation into multiple HDF5 files which are later combined. This is useful to avoid running out of RAM with large grids. \n",
    "\n",
    "We can also set the output type - in this case it is \"photometry\", but if instead we made in \"spectra\", we would generate a library of spectra which we could infer from as well. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "33bffaf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-11-05 11:44:45,446 | synference | INFO     | Checking parameters inside create_matched_galaxies.\n",
      "2025-11-05 11:44:45,450 | synference | INFO     | 1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preparing galaxy batches: 100%|██████████| 1000/1000 [00:00<00:00, 274209.21it/s]\n",
      "Creating galaxies: 100%|██████████| 1/1 [00:41<00:00, 41.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-11-05 11:45:27,067 | synference | INFO     | Created 1000 galaxies.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing parameters: 100%|██████████| 11/11 [00:00<00:00, 7105.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-11-05 11:45:27,075 | synference | INFO     | Finished creating galaxies.\n",
      "2025-11-05 11:45:27,075 | synference | INFO     | Created 1000 galaxies for base sps_Euclid_test\n",
      "2025-11-05 11:45:27,079 | synference | INFO     | Creating pipeline.\n",
      "2025-11-05 11:45:27,080 | synference | INFO     | Processing all galaxies in a single batch.\n",
      "2025-11-05 11:45:27,080 | synference | INFO     | Running in single-node mode.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "InconsistentArguments",
     "evalue": "Can't use multiple threads without OpenMP support.  Install with: `WITH_OPENMP=1 pip install .` ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInconsistentArguments\u001b[0m                     Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mbasis\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_mock_cat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43memission_model_key\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43memission_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mout_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgrid_Euclid_test\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mout_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43moverwrite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_proc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10_000\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparameter_transforms_to_save\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparam_transforms_to_save\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcat_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mphotometry\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msupp_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/PhD/synference/src/synference/grid.py:3096\u001b[0m, in \u001b[0;36mGalaxyBasis.create_mock_library\u001b[0;34m(self, out_name, log_stellar_masses, emission_model_key, out_dir, n_proc, overwrite, verbose, batch_size, parameter_transforms_to_save, cat_type, compile_grid, multi_node, spectra_to_save, em_lines_to_save, **extra_analysis_functions)\u001b[0m\n\u001b[1;32m   3093\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   3094\u001b[0m     galaxy_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 3096\u001b[0m \u001b[43mcombined_basis\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess_bases\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3097\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_proc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_proc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3098\u001b[0m \u001b[43m    \u001b[49m\u001b[43moverwrite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moverwrite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3099\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3100\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3101\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmulti_node\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmulti_node\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3102\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgalaxies_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgalaxy_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3103\u001b[0m \u001b[43m    \u001b[49m\u001b[43mspectra_to_save\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mspectra_to_save\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3104\u001b[0m \u001b[43m    \u001b[49m\u001b[43mem_lines_to_save\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mem_lines_to_save\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3105\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mextra_analysis_functions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3106\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m compile_grid:\n\u001b[1;32m   3109\u001b[0m     \u001b[38;5;66;03m# Make code wait until all bases are processed\u001b[39;00m\n\u001b[1;32m   3110\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCompiling the grid after processing bases.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Documents/PhD/synference/src/synference/grid.py:3323\u001b[0m, in \u001b[0;36mCombinedBasis.process_bases\u001b[0;34m(self, n_proc, overwrite, verbose, batch_size, multi_node, galaxies_mask, spectra_to_save, em_lines_to_save, **extra_analysis_functions)\u001b[0m\n\u001b[1;32m   3321\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreated \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(galaxies)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m galaxies for base \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbase\u001b[38;5;241m.\u001b[39mmodel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   3322\u001b[0m \u001b[38;5;66;03m# Process the galaxies\u001b[39;00m\n\u001b[0;32m-> 3323\u001b[0m \u001b[43mbase\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess_galaxies\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3324\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgalaxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3325\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mbase\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_name\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.hdf5\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3326\u001b[0m \u001b[43m    \u001b[49m\u001b[43mout_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mout_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3327\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_proc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_proc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3328\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3329\u001b[0m \u001b[43m    \u001b[49m\u001b[43msave\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   3330\u001b[0m \u001b[43m    \u001b[49m\u001b[43memission_model_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbase_emission_model_keys\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3331\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3332\u001b[0m \u001b[43m    \u001b[49m\u001b[43moverwrite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moverwrite\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3333\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmulti_node\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmulti_node\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3334\u001b[0m \u001b[43m    \u001b[49m\u001b[43mspectra_to_save\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mspectra_to_save\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3335\u001b[0m \u001b[43m    \u001b[49m\u001b[43mem_lines_to_save\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mem_lines_to_save\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3336\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mextra_analysis_functions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3337\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/PhD/synference/src/synference/grid.py:2523\u001b[0m, in \u001b[0;36mGalaxyBasis.process_galaxies\u001b[0;34m(self, galaxies, out_name, out_dir, n_proc, verbose, save, emission_model_keys, batch_galaxies, batch_size, overwrite, multi_node, spectra_to_save, em_lines_to_save, igm, **extra_analysis_functions)\u001b[0m\n\u001b[1;32m   2519\u001b[0m     \u001b[38;5;66;03m# logger.debug(f\"SIZE: {size}, RANK: {rank}\")\u001b[39;00m\n\u001b[1;32m   2520\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2521\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRunning in single-node mode.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 2523\u001b[0m pipeline \u001b[38;5;241m=\u001b[39m \u001b[43mPipeline\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2524\u001b[0m \u001b[43m    \u001b[49m\u001b[43memission_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43memission_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2525\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnthreads\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_proc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2526\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2527\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcomm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcomm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2528\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2530\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mall_parameters\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m   2531\u001b[0m     pipeline\u001b[38;5;241m.\u001b[39madd_analysis_func(\n\u001b[1;32m   2532\u001b[0m         \u001b[38;5;28;01mlambda\u001b[39;00m gal, key\u001b[38;5;241m=\u001b[39mkey: gal\u001b[38;5;241m.\u001b[39mall_params[key],\n\u001b[1;32m   2533\u001b[0m         result_key\u001b[38;5;241m=\u001b[39mkey,\n\u001b[1;32m   2534\u001b[0m     )\n",
      "File \u001b[0;32m~/Documents/PhD/synthesizer/synthesizer/src/synthesizer/pipeline/pipeline.py:192\u001b[0m, in \u001b[0;36mPipeline.__init__\u001b[0;34m(self, emission_model, nthreads, comm, verbose, report_memory)\u001b[0m\n\u001b[1;32m    190\u001b[0m \u001b[38;5;66;03m# Check if we can use OpenMP\u001b[39;00m\n\u001b[1;32m    191\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnthreads \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m check_openmp():\n\u001b[0;32m--> 192\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mInconsistentArguments(\n\u001b[1;32m    193\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCan\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt use multiple threads without OpenMP support. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    194\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m Install with: `WITH_OPENMP=1 pip install .`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    195\u001b[0m     )\n\u001b[1;32m    197\u001b[0m \u001b[38;5;66;03m# Define flags for what we will do\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_do_los_optical_depths \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[0;31mInconsistentArguments\u001b[0m: Can't use multiple threads without OpenMP support.  Install with: `WITH_OPENMP=1 pip install .` "
     ]
    }
   ],
   "source": [
    "basis.create_mock_library(\n",
    "    emission_model_key=emission_key,\n",
    "    out_name=\"grid_Euclid_test\",\n",
    "    out_dir=\"./\",\n",
    "    overwrite=True,\n",
    "    n_proc=1,\n",
    "    verbose=False,\n",
    "    batch_size=10_000,\n",
    "    parameter_transforms_to_save=param_transforms_to_save,\n",
    "    cat_type=\"photometry\",\n",
    "    **supp_params,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17c98964",
   "metadata": {},
   "source": [
    "## Spectroscopic Grid Creation\n",
    "\n",
    "We can also build a library of spectra, which we can train a model from using an embedding network. At the default setting, the spectra would be at the wavelength range and resolution of our SPS grid, which is likely higher than required. We can change the wavelength array on our instrument or grid to a more reasonable choice.\n",
    "```python\n",
    "from unyt import Angstrom\n",
    "\n",
    "from synference import generate_constant_R\n",
    "\n",
    "new_lam = generate_constant_R(300, start=100 * Angstrom, stop=100_000 * Angstrom)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1911b7cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "basis.create_mock_library(\n",
    "    emission_model_key=emission_key,\n",
    "    out_name=\"spectral_grid_Euclid_test\",\n",
    "    out_dir=\"./\",\n",
    "    overwrite=True,\n",
    "    n_proc=4,\n",
    "    verbose=False,\n",
    "    batch_size=10_000,\n",
    "    parameter_transforms_to_save=param_transforms_to_save,\n",
    "    cat_type=\"spectra\",\n",
    "    **supp_params,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "astro",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
