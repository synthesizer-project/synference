{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b420dbed",
   "metadata": {},
   "source": [
    "## Complex Library Generation\n",
    "\n",
    "In addition to the basic library generation demonstrated previously, Synference also supports more complex scenarios, such as grids with more supplementary parameters, custom observation transformations, and more complicated model setups. Below is an example of how to generate a more complex model library using Synference.\n",
    "\n",
    "\n",
    "Firstly we'll just import the necessary modules and set up the synthesizer model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c57bea49",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from astropy.cosmology import Planck18 as cosmo\n",
    "from synthesizer.emission_models.attenuation import (\n",
    "    Calzetti2000,\n",
    ")  # noqa\n",
    "from synthesizer.emission_models.dust.emission import Greybody, IR_templates  # noqa\n",
    "from synthesizer.emission_models.stellar.pacman_model import (\n",
    "    PacmanEmission,\n",
    ")  # noqa\n",
    "from synthesizer.grid import Grid\n",
    "from synthesizer.instruments import FilterCollection, Instrument\n",
    "from synthesizer.parametric import ZDist\n",
    "from tqdm import tqdm\n",
    "from unyt import K"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "859bb90c",
   "metadata": {},
   "source": [
    "And our Synference components - the GalaxyBasis class, and some utility functions we will use later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fe82d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "from synference import (\n",
    "    GalaxyBasis,\n",
    "    calculate_balmer_decrement,\n",
    "    calculate_beta,\n",
    "    calculate_colour,\n",
    "    calculate_d4000,\n",
    "    calculate_mass_weighted_age,\n",
    "    calculate_muv,\n",
    "    calculate_sfh_quantile,\n",
    "    calculate_surviving_mass,\n",
    "    draw_from_hypercube,\n",
    "    generate_random_DB_sfh,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70cb723f",
   "metadata": {},
   "source": [
    "Firstly we'll set up our Synthesizer similarly to before - see the basic library generation example for more details.\n",
    "\n",
    "For this example we'll use a set of filter used in wide area surveys, including VISTA, Subaru Hyper Suprime-Cam, Euclid, and Spitzer IRAC.\n",
    "\n",
    "Note that if you're running this step on a cluster node without internet access, you'll need to create the instrument file beforehand and pass in the HDF5 file path insead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfc44396",
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_codes = [\n",
    "    \"Paranal/VISTA.Z\",\n",
    "    \"Paranal/VISTA.Y\",\n",
    "    \"Paranal/VISTA.J\",\n",
    "    \"Paranal/VISTA.H\",\n",
    "    \"Paranal/VISTA.Ks\",\n",
    "    \"Subaru/HSC.g\",\n",
    "    \"Subaru/HSC.r\",\n",
    "    \"Subaru/HSC.i\",\n",
    "    \"Subaru/HSC.z\",\n",
    "    \"Subaru/HSC.Y\",\n",
    "    \"CFHT/MegaCam.u\",\n",
    "    \"CFHT/MegaCam.g\",\n",
    "    \"CFHT/MegaCam.r\",\n",
    "    \"CFHT/MegaCam.i\",\n",
    "    \"CFHT/MegaCam.z\",\n",
    "    \"Euclid/VIS.vis\",\n",
    "    \"Euclid/NISP.Y\",\n",
    "    \"Euclid/NISP.J\",\n",
    "    \"Euclid/NISP.H\",\n",
    "    \"Spitzer/IRAC.I1\",\n",
    "    \"Spitzer/IRAC.I2\",\n",
    "]\n",
    "\n",
    "filterset = FilterCollection(filter_codes=filter_codes)\n",
    "\n",
    "instrument = Instrument(\"EuclidDeep\", filters=filterset)\n",
    "\n",
    "grid = Grid(\"bpass-2.2.1-bin_chabrier03-0.1,300.0_cloudy-c23.01-sps\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8593eca3",
   "metadata": {},
   "source": [
    "Now we can configure our model, which we will make more complex. Firstly we define the prior ranges for our main galaxy parameters.\n",
    "\n",
    "This model will use a more complex dust attenuation model, with variable UV slope and 2175A bump strength, as well as a variable escape fraction for ionizing photons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b6f21d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_models = 1000\n",
    "\n",
    "redshift = (0.01, 14)\n",
    "masses = (4, 12)  # log(M/Msun)=4 to log(M/Msun)=12\n",
    "logAv = (-3, 0.7)  # Av=0.001 to Av=5\n",
    "log_zmet = (-4, -1.39)  # Z=0.0001 to Z=0.04\n",
    "fesc = (0.0, 1.0)  # Fraction of ionizing photons that escape the galaxy\n",
    "slope = (-0.4, 1.1)  # UV slope modification to Calzetti law\n",
    "bump_strength = (0.0, 3.0)  # 2175A bump strength in Calzetti law\n",
    "\n",
    "\n",
    "prior_ranges = {\n",
    "    \"redshift\": redshift,\n",
    "    \"log_masses\": masses,\n",
    "    \"log_Av\": logAv,  # Av in magnitudes\n",
    "    \"log_zmet\": log_zmet,\n",
    "    \"fesc\": fesc,\n",
    "    \"slope\": slope,\n",
    "    \"bump_strength\": bump_strength,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8baabcb9",
   "metadata": {},
   "source": [
    "### Star Formation History\n",
    "\n",
    "We'll use a non-parametric 'Dense Basis' SFH (Iyer et al. 2019), where we model the time at which different quantiles of stellar mass formed. Synference provides a helper module for generating these Star Formation Histories.\n",
    "\n",
    "We will use 3 quantiles, e.g. $t_{25}, t_{50}, t_{75}$, so we will simply add three dummy parameters to our LHC sampling of the parameter space, and then replace them afterward. We need to set the concentration parameter $\\alpha$ for the Dirichilet prior on the SFH, for which we will use $\\alpha=3$. This controls the correlation between different SFH quantiles, where lower values will have more rapdily varying star formation histories. We also set a prior on the recent SFR, in terms of the sSFR, which normalizes by the stellar mass. This allows for a range from quiescent to highly star-forming galaxies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c081244d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tx_alpha = 3\n",
    "\n",
    "for i in range(3):\n",
    "    j = 100 * (i + 1) / (4)\n",
    "    prior_ranges[f\"sfh_quantile_{j:.0f}\"] = (0, 1)\n",
    "\n",
    "prior_ranges[\"ssfr\"] = (-14, -7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2f71bbe",
   "metadata": {},
   "source": [
    "Now we will sample these parameters from our hypercube. Note that we set a log prior on dust attenuation $A_V$, but we want to sample linear dust attenuation, so we get the model to 'unlog' the parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f9938b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_param_dict = draw_from_hypercube(prior_ranges, N_models, unlog_keys=[\"log_Av\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd2afc37",
   "metadata": {},
   "source": [
    "Now we will create our metallicity distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6207629",
   "metadata": {},
   "outputs": [],
   "source": [
    "Z_dists = [\n",
    "    ZDist.DeltaConstant(log10metallicity=log_z)\n",
    "    for log_z in tqdm(all_param_dict[\"log_zmet\"], desc=\"Creating ZDist\")\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "640cbfd6",
   "metadata": {},
   "source": [
    "Now we can create the star-formation history. We are using a specific prior in specific star-formation rate here, but we need to provide log SFR to the function, so we calculate this inside the loop. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6df824bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw SFH params from prior\n",
    "sfh_models = []\n",
    "for i in tqdm(range(N_models), desc=\"Generating SFH models\"):\n",
    "    z = all_param_dict[\"redshift\"][i]\n",
    "    logmass = all_param_dict[\"log_masses\"][i]\n",
    "    logssfr = all_param_dict[\"ssfr\"][i]\n",
    "    logsfr = logmass + logssfr\n",
    "    sfh, tx = generate_random_DB_sfh(\n",
    "        Nparam=3,\n",
    "        tx_alpha=tx_alpha,\n",
    "        redshift=z,\n",
    "        logsfr=logsfr,\n",
    "        logmass=logmass,\n",
    "    )\n",
    "    for j in range(3):\n",
    "        all_param_dict[f\"sfh_quantile_{100 * (j + 1) / (3 + 1):.0f}\"][i] = tx[j]\n",
    "    sfh_models.append(sfh)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33192b06",
   "metadata": {},
   "source": [
    "Now we will set up two functions, which are used to convert parameters from one form to another inside the model, and allow the SFH model to be serialized and re-created. This is a flexible system which should allow complex transformations.\n",
    "\n",
    "These are required because synference automatically looks for varying parameters which are stored on the Synthesizer galaxies and emitters, but if they are in a complex form (e.g. the Dense Basis tuple), then we need to explain how to understand them to the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93f3a653",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_db_tuple(params):\n",
    "    \"\"\"Constructs the DenseBasis tuple from the SFH.\"\"\"\n",
    "    nquant = 0\n",
    "    for key in params:\n",
    "        if key.startswith(\"sfh_quantile_\"):\n",
    "            nquant += 1\n",
    "\n",
    "    mass_quantiles = np.linspace(0, 1, nquant + 2)[1:-1]  # Exclude the 0 and 1 quantiles\n",
    "\n",
    "    db_tuple = [params[\"log_mass\"], params[\"log_sfr\"], nquant] + [\n",
    "        params[f\"sfh_quantile_{int(q * 100)}\"] for q in mass_quantiles\n",
    "    ]\n",
    "    return db_tuple  # Return a tuple of (log_mass, SFR, nquant, [quantiles...])\n",
    "\n",
    "\n",
    "def db_sf_convert(param, param_dict, Nparam_SFH=3):\n",
    "    \"\"\"Converts from a DenseBasis tuple back to parameters.\"\"\"\n",
    "    db_tuple = param_dict[\"db_tuple\"]\n",
    "    # dp_tuple has the folliwng\n",
    "    # mass, sfr, tx_alpha, *sfh_quantiles\n",
    "    if param.startswith(\"sfh_quantile_\"):\n",
    "        # Convert the SFH quantile parameters to the Dense Basis SFH format\n",
    "        j = int(np.round(int(param.split(\"_\")[-1]) / 100 * (Nparam_SFH + 1)))\n",
    "        return db_tuple[j + 2]  # +3 because first three are mass, sfr, tx_alpha\n",
    "    elif param == \"log_sfr\":\n",
    "        # Convert log_sfr to the Dense Basis SFH format\n",
    "        return db_tuple[1]\n",
    "    elif param == \"log_masses\":\n",
    "        # Convert log_masses to the Dense Basis SFH format\n",
    "        return db_tuple[0]\n",
    "    elif param == \"tx_alpha\":\n",
    "        # Convert tx_alpha to the Dense Basis SFH format\n",
    "        return db_tuple[2]\n",
    "    elif param == \"log_ssfr\":\n",
    "        # Convert log_ssfr to the Dense Basis SFH format\n",
    "        return db_tuple[1] - db_tuple[0]\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown parameter {param.str} in db_tuple conversion.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87d92a48",
   "metadata": {},
   "source": [
    "Now we can set up some parameter transformations. These are for when we want to sample a parameter which is not what is used directly in Synthesizer. The simplest example is our use of the V-band attenuation $A_V$, whereas in reality to generate a model we must provide the V-band optical depth $\\tau_V$, to Synthesizer. So we provide a function to Synference to allow it to do this conversion, which is simply $A_V = \\tau_V * 1.086$\n",
    "\n",
    "These generally take the form of a dictionary, where the key is the parameter name required by the Synthesizer model, and value is a two component tuple, where the first value is a new name (or a list of new names, for multiple parameters), and the second value the conversion function.\n",
    "\n",
    "We will also save the inverse functions in a separate dictionary. These are only used if we want to recreate this simulator later, to generate SEDs from a set of input parameters. This is useful to recover SEDs for observations when performing inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3302328f",
   "metadata": {},
   "outputs": [],
   "source": [
    "alt_parametrizations = {\n",
    "    \"tau_v\": (\"Av\", lambda x: x[\"tau_v\"] * 1.086),\n",
    "    \"db_tuple\": (\n",
    "        [\"log_sfr\"] + [f\"sfh_quantile_{100 * (j + 1) / (3 + 1):.0f}\" for j in range(3)],\n",
    "        db_sf_convert,\n",
    "    ),\n",
    "}\n",
    "\n",
    "param_transforms_to_save = {\n",
    "    \"tau_v\": lambda x: x[\"Av\"] / 1.086,\n",
    "    \"db_tuple\": make_db_tuple,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "535aae99",
   "metadata": {},
   "source": [
    "### Emission Models\n",
    "\n",
    "Now we will set up our complex emission model which supports our priors, with variable escape fraction and flexible attenuation law.\n",
    "\n",
    "The basic concept is simple: **Any emission model parameter set with a string, rather than an explicit value, will be inherited from the emission model, or emitter.** So in this case, for 'tau_v', 'fesc', 'bump_strength', and 'slope', Synthesizer will look for these parameters to be set on the individual Galaxy or Star instances.\n",
    "\n",
    "We will also set the emission key we will save, which is the root of the emission model, named 'total'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fb0b870",
   "metadata": {},
   "outputs": [],
   "source": [
    "dust_emission = Greybody(temperature=40 * K, emissivity=1.5)\n",
    "dust_curve = Calzetti2000(slope=\"slope\", ampl=\"bump_strength\")\n",
    "\n",
    "print(\"Creating emission model.\")\n",
    "emission_model = PacmanEmission(\n",
    "    grid=grid, tau_v=\"tau_v\", dust_curve=dust_curve, dust_emission=dust_emission, fesc=\"fesc\"\n",
    ")\n",
    "\n",
    "emission_key = emission_model.label\n",
    "\n",
    "print(f\"Root emission model label is: {emission_key}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27558101",
   "metadata": {},
   "source": [
    "To ensure these parameters are set on each galaxy, we will create our final input, the ```galaxy_params``` dictionary. This is simply a dictionary of parameter name and value array pairs for every galaxy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97a1d91e",
   "metadata": {},
   "outputs": [],
   "source": [
    "galaxy_params = {\n",
    "    \"fesc\": all_param_dict[\"fesc\"],\n",
    "    \"tau_v\": all_param_dict[\"Av\"] / 1.086,\n",
    "    \"bump_strength\": all_param_dict[\"bump_strength\"],\n",
    "    \"slope\": all_param_dict[\"slope\"],\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27812cf5",
   "metadata": {},
   "source": [
    "Now we can instantiate the GalaxyBasis, into which we will pass these inputs. This won't do much until we call the correct function to build the grid. Note that we set 'build_grid' = False, because we have already generated our full grid of parameters. If we wanted we could also pass in a smaller set of parameter values instead, and set build_grid=True, and the code would generate all the combinations of those parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7cc6d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "basis = GalaxyBasis(\n",
    "    model_name=\"sps_Euclid_test\",\n",
    "    redshifts=all_param_dict[\"redshift\"],\n",
    "    grid=grid,\n",
    "    emission_model=emission_model,\n",
    "    sfhs=sfh_models,\n",
    "    cosmo=cosmo,\n",
    "    instrument=instrument,\n",
    "    metal_dists=Z_dists,\n",
    "    galaxy_params=galaxy_params,\n",
    "    alt_parametrizations=alt_parametrizations,\n",
    "    redshift_dependent_sfh=True,\n",
    "    build_grid=False,\n",
    "    log_stellar_masses=all_param_dict[\"log_masses\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05cc557d",
   "metadata": {},
   "source": [
    "Something else we can do here to improve the utility of our model is add more parameters to be saved and stored. Synference provides a set of these parameters, to save things like the surviving stellar mass, UV magnitude, $\\beta$ slope, D4000 break strength, UVJ colors, etc. We can see the full list here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaad90a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from synference import SUPP_FUNCTIONS\n",
    "\n",
    "SUPP_FUNCTIONS()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48bc8091",
   "metadata": {},
   "source": [
    "We can pass in these functions to our ```GalaxyBasis.create_mock_cat``` function, and they will be run for every galaxy and the output stored in the grid.\n",
    "\n",
    "The functions should take the galaxy as the first argument, and then the following arguments (if any) will be set by position as demonstrated below. The keys will be the parameter names. The return should be either a single value (float, string, unyt_quantity) or a dictionary which maps to the emission model names (e.g to record $\\beta$ slope for different components)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa246302",
   "metadata": {},
   "outputs": [],
   "source": [
    "supp_params = {\n",
    "    \"mUV\": (calculate_muv, cosmo),\n",
    "    \"sfh_quant_25\": (calculate_sfh_quantile, 0.25, True),  # Calculate SFH quantile at 25%\n",
    "    \"sfh_quant_50\": (calculate_sfh_quantile, 0.50, True),  # Calculate SFH quantile at 50%\n",
    "    \"sfh_quant_75\": (calculate_sfh_quantile, 0.75, True),  # Calculate SFH quantile at 75%\n",
    "    \"UV\": (calculate_colour, \"U\", \"V\", emission_key, True),  # Calculate UV colour (rest-frame)\n",
    "    \"VJ\": (calculate_colour, \"V\", \"J\", emission_key, True),  # Calculate VJ colour (rest-frame)\n",
    "    \"log_surviving_mass\": (calculate_surviving_mass, grid),  # Calculate surviving mass\n",
    "    \"d4000\": (calculate_d4000, emission_key),  # Calculate D4000 using the emission model\n",
    "    \"beta\": (calculate_beta, emission_key),\n",
    "    \"balmer_decrement\": (calculate_balmer_decrement, emission_key),\n",
    "    \"mass_weighted_age\": calculate_mass_weighted_age,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7130ca9d",
   "metadata": {},
   "source": [
    "### Grid Generation\n",
    "\n",
    "Now we can run the final method to create the output catalogue. There are several things to note here. We can set the number of processes to use, to make use of multiple threads. We can also set the batch size, which will split the generation into multiple HDF5 files which are later combined. This is useful to avoid running out of RAM with large grids. \n",
    "\n",
    "We can also set the output type - in this case it is \"photometry\", but if instead we made in \"spectra\", we would generate a library of spectra which we could infer from as well. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33bffaf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "basis.create_mock_cat(\n",
    "    emission_model_key=emission_key,\n",
    "    out_name=\"grid_Euclid_test\",\n",
    "    out_dir=\"./\",\n",
    "    overwrite=True,\n",
    "    n_proc=4,\n",
    "    verbose=False,\n",
    "    batch_size=10_000,\n",
    "    parameter_transforms_to_save=param_transforms_to_save,\n",
    "    cat_type=\"photometry\",\n",
    "    **supp_params,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17c98964",
   "metadata": {},
   "source": [
    "## Spectroscopic Grid Creation\n",
    "\n",
    "We can also build a library of spectra, which we can train a model from using an embedding network. At the default setting, the spectra would be at the wavelength range and resolution of our SPS grid, which is likely higher than required. We can change the wavelength array on our instrument or grid to a more reasonable choice.\n",
    "```python\n",
    "from unyt import Angstrom\n",
    "\n",
    "from synference import generate_constant_R\n",
    "\n",
    "new_lam = generate_constant_R(300, start=100 * Angstrom, stop=100_000 * Angstrom)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1911b7cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "basis.create_mock_cat(\n",
    "    emission_model_key=emission_key,\n",
    "    out_name=\"spectral_grid_Euclid_test\",\n",
    "    out_dir=\"./\",\n",
    "    overwrite=True,\n",
    "    n_proc=4,\n",
    "    verbose=False,\n",
    "    batch_size=10_000,\n",
    "    parameter_transforms_to_save=param_transforms_to_save,\n",
    "    cat_type=\"spectra\",\n",
    "    **supp_params,\n",
    ")"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
